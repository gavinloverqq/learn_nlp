{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "李宏毅视频\n",
    "https://www.youtube.com/watch?v=kk6DqWreLeU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 强化学习的步骤\n",
    "- 定义一个actor，actor实现的是个分类任务，当前状况s输入，输出是动作actor，actor根据概率分布，sample一个结果出来。\n",
    "- 定义loss，\n",
    "- 能做的动作，不能做的动作，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前面的动作，对后面的影响很大，因此reward更大，\n",
    "- 动作的reward不能都是正的，有些动作不能做，因此需要有正有负，因此要-b\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy gradient\n",
    "- 初始化一个网络actor0\n",
    "- 用当前的actor去产生动作，获取一系列(s, a)\n",
    "- 计算每一步产生的分数A，合并之后是loss\n",
    "- 根据当前的loss去更新参数，得到新的actor，\n",
    "- 持续迭代，得到最终的loss\n",
    "- 每次更新只更新一次，因为新的actor产生的（s, a）只能用来更新当前的actor，比如初级的人下棋，一个平庸的招数是更符合下棋的actor的水平的，当时更强水平的人可能需要下出风险更高的招，这样收益才能更高，但是这个招对于初级水平的actor，这招拿到的reward却更低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-7.png)\n",
    "![Alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## off-policy vs on-policy\n",
    "- train的actor和interact的actor是不是一样的，\n",
    "- 一样的就是on-policy\n",
    "- 不一样的就是off-policy\n",
    "- (my comments) 区别是使用同一个数据集还是上面的说法？\n",
    "\n",
    "## PPO\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-9.png)\n",
    "![Alt text](image-10.png)\n",
    "![Alt text](image-11.png)\n",
    "![Alt text](image-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC vs TD\n",
    "- ![Alt text](image-13.png)\n",
    "- ![Alt text](image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-16.png)\n",
    "\n",
    "- 定义一个额外的reward，因为大多数分数都是0，没有区分度，这个额外的reward就是reward shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-17.png)\n",
    " \n",
    "- 人工定义的reward很困难，往往做出意想不到的动作\n",
    "- 比如上面图片中，机器人可能把所有人类监禁起来已获得最高的reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-18.png)\n",
    "\n",
    "![Alt text](image-19.png)\n",
    "\n",
    "![Alt text](image-20.png)\n",
    "\n",
    "![Alt text](image-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRL\n",
    "- 找个专家来做一遍动作\n",
    "- 定义一个reward function，让actor的得分不超过专家\n",
    "- actor尝试学习到参数，基于当时的reward function来让reward最大\n",
    "- 输出reward function 和 actor\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
