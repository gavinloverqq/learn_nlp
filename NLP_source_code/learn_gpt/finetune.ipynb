{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['all_proxy'] = \"socks5://127.0.0.1:10808\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/frog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b7a2c3cae54f4e9eb6f81cc9b8ed9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-33M\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/frog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-3579c015ced2fa53.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/frog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-657b7992ea4cc6bb.arrow\n",
      "Loading cached processed dataset at /home/frog/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ced6a81e5227fe5c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (12460, 2)\n",
      "Validation: (500, 2)\n",
      "Test: (1500, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb. init(mode=\"disabled\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b749bc2beef47ab8b7e0b6fe8c39ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 49.5, 'learning_rate': 9.99e-05, 'epoch': 0.0}\n",
      "{'loss': 44.75, 'learning_rate': 9.98e-05, 'epoch': 0.0}\n",
      "{'loss': 40.75, 'learning_rate': 9.970000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 36.5, 'learning_rate': 9.960000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 34.75, 'learning_rate': 9.95e-05, 'epoch': 0.0}\n",
      "{'loss': 33.75, 'learning_rate': 9.94e-05, 'epoch': 0.0}\n",
      "{'loss': 31.125, 'learning_rate': 9.93e-05, 'epoch': 0.0}\n",
      "{'loss': 30.875, 'learning_rate': 9.92e-05, 'epoch': 0.01}\n",
      "{'loss': 28.75, 'learning_rate': 9.910000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 27.25, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 27.125, 'learning_rate': 9.89e-05, 'epoch': 0.01}\n",
      "{'loss': 26.25, 'learning_rate': 9.88e-05, 'epoch': 0.01}\n",
      "{'loss': 24.75, 'learning_rate': 9.87e-05, 'epoch': 0.01}\n",
      "{'loss': 24.375, 'learning_rate': 9.86e-05, 'epoch': 0.01}\n",
      "{'loss': 23.375, 'learning_rate': 9.850000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 22.375, 'learning_rate': 9.84e-05, 'epoch': 0.01}\n",
      "{'loss': 22.125, 'learning_rate': 9.83e-05, 'epoch': 0.01}\n",
      "{'loss': 21.375, 'learning_rate': 9.82e-05, 'epoch': 0.01}\n",
      "{'loss': 21.25, 'learning_rate': 9.81e-05, 'epoch': 0.01}\n",
      "{'loss': 19.25, 'learning_rate': 9.8e-05, 'epoch': 0.01}\n",
      "{'loss': 17.5, 'learning_rate': 9.790000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 17.625, 'learning_rate': 9.78e-05, 'epoch': 0.01}\n",
      "{'loss': 16.75, 'learning_rate': 9.77e-05, 'epoch': 0.01}\n",
      "{'loss': 14.375, 'learning_rate': 9.76e-05, 'epoch': 0.02}\n",
      "{'loss': 14.1875, 'learning_rate': 9.75e-05, 'epoch': 0.02}\n",
      "{'loss': 12.25, 'learning_rate': 9.74e-05, 'epoch': 0.02}\n",
      "{'loss': 13.0, 'learning_rate': 9.730000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 10.375, 'learning_rate': 9.72e-05, 'epoch': 0.02}\n",
      "{'loss': 9.4375, 'learning_rate': 9.71e-05, 'epoch': 0.02}\n",
      "{'loss': 9.8125, 'learning_rate': 9.7e-05, 'epoch': 0.02}\n",
      "{'loss': 7.375, 'learning_rate': 9.69e-05, 'epoch': 0.02}\n",
      "{'loss': 5.9688, 'learning_rate': 9.680000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 7.8438, 'learning_rate': 9.67e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6875, 'learning_rate': 9.66e-05, 'epoch': 0.02}\n",
      "{'loss': 8.375, 'learning_rate': 9.65e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0625, 'learning_rate': 9.64e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9688, 'learning_rate': 9.63e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8438, 'learning_rate': 9.620000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8438, 'learning_rate': 9.61e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8125, 'learning_rate': 9.6e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6875, 'learning_rate': 9.59e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6562, 'learning_rate': 9.58e-05, 'epoch': 0.03}\n",
      "{'loss': 4.625, 'learning_rate': 9.57e-05, 'epoch': 0.03}\n",
      "{'loss': 4.625, 'learning_rate': 9.56e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5312, 'learning_rate': 9.55e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4688, 'learning_rate': 9.54e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5, 'learning_rate': 9.53e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5, 'learning_rate': 9.52e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5, 'learning_rate': 9.51e-05, 'epoch': 0.03}\n",
      "{'loss': 4.375, 'learning_rate': 9.5e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4062, 'learning_rate': 9.49e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4062, 'learning_rate': 9.48e-05, 'epoch': 0.03}\n",
      "{'loss': 4.875, 'learning_rate': 9.47e-05, 'epoch': 0.03}\n",
      "{'loss': 4.375, 'learning_rate': 9.46e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3125, 'learning_rate': 9.449999999999999e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5312, 'learning_rate': 9.44e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3438, 'learning_rate': 9.43e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2188, 'learning_rate': 9.42e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2812, 'learning_rate': 9.41e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9688, 'learning_rate': 9.4e-05, 'epoch': 0.04}\n",
      "{'loss': 4.25, 'learning_rate': 9.39e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1562, 'learning_rate': 9.38e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1562, 'learning_rate': 9.370000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 4.625, 'learning_rate': 9.360000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0938, 'learning_rate': 9.350000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 5.2188, 'learning_rate': 9.340000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0938, 'learning_rate': 9.33e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0312, 'learning_rate': 9.320000000000002e-05, 'epoch': 0.04}\n",
      "{'loss': 3.9844, 'learning_rate': 9.310000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.9219, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.8906, 'learning_rate': 9.290000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.8438, 'learning_rate': 9.28e-05, 'epoch': 0.05}\n",
      "{'loss': 3.8281, 'learning_rate': 9.27e-05, 'epoch': 0.05}\n",
      "{'loss': 3.8438, 'learning_rate': 9.260000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.7812, 'learning_rate': 9.250000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.7812, 'learning_rate': 9.240000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.75, 'learning_rate': 9.230000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.6719, 'learning_rate': 9.22e-05, 'epoch': 0.05}\n",
      "{'loss': 4.9375, 'learning_rate': 9.21e-05, 'epoch': 0.05}\n",
      "{'loss': 3.75, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.625, 'learning_rate': 9.190000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4688, 'learning_rate': 9.180000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5625, 'learning_rate': 9.17e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4688, 'learning_rate': 9.16e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3281, 'learning_rate': 9.15e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4219, 'learning_rate': 9.140000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3125, 'learning_rate': 9.130000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5156, 'learning_rate': 9.120000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.2188, 'learning_rate': 9.11e-05, 'epoch': 0.06}\n",
      "{'loss': 3.2344, 'learning_rate': 9.1e-05, 'epoch': 0.06}\n",
      "{'loss': 3.25, 'learning_rate': 9.090000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.2188, 'learning_rate': 9.080000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.7812, 'learning_rate': 9.070000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.1562, 'learning_rate': 9.06e-05, 'epoch': 0.06}\n",
      "{'loss': 2.9688, 'learning_rate': 9.05e-05, 'epoch': 0.06}\n",
      "{'loss': 3.0781, 'learning_rate': 9.04e-05, 'epoch': 0.06}\n",
      "{'loss': 2.9531, 'learning_rate': 9.030000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.0469, 'learning_rate': 9.020000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 3.0781, 'learning_rate': 9.010000000000001e-05, 'epoch': 0.06}\n",
      "{'loss': 2.7969, 'learning_rate': 9e-05, 'epoch': 0.06}\n",
      "{'loss': 2.9375, 'learning_rate': 8.99e-05, 'epoch': 0.06}\n",
      "{'loss': 2.7812, 'learning_rate': 8.98e-05, 'epoch': 0.07}\n",
      "{'loss': 2.7188, 'learning_rate': 8.970000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 2.9375, 'learning_rate': 8.960000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 2.6406, 'learning_rate': 8.950000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 2.6875, 'learning_rate': 8.94e-05, 'epoch': 0.07}\n",
      "{'loss': 2.5625, 'learning_rate': 8.93e-05, 'epoch': 0.07}\n",
      "{'loss': 2.5781, 'learning_rate': 8.92e-05, 'epoch': 0.07}\n",
      "{'loss': 2.625, 'learning_rate': 8.910000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 2.4531, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 4.1875, 'learning_rate': 8.89e-05, 'epoch': 0.07}\n",
      "{'loss': 2.4844, 'learning_rate': 8.88e-05, 'epoch': 0.07}\n",
      "{'loss': 2.9062, 'learning_rate': 8.87e-05, 'epoch': 0.07}\n",
      "{'loss': 2.5, 'learning_rate': 8.86e-05, 'epoch': 0.07}\n",
      "{'loss': 2.4531, 'learning_rate': 8.850000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 2.5312, 'learning_rate': 8.840000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 2.4062, 'learning_rate': 8.83e-05, 'epoch': 0.08}\n",
      "{'loss': 2.75, 'learning_rate': 8.82e-05, 'epoch': 0.08}\n",
      "{'loss': 2.3906, 'learning_rate': 8.81e-05, 'epoch': 0.08}\n",
      "{'loss': 2.375, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.2188, 'learning_rate': 8.790000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.2188, 'learning_rate': 8.78e-05, 'epoch': 0.08}\n",
      "{'loss': 2.1719, 'learning_rate': 8.77e-05, 'epoch': 0.08}\n",
      "{'loss': 2.5469, 'learning_rate': 8.76e-05, 'epoch': 0.08}\n",
      "{'loss': 2.1719, 'learning_rate': 8.75e-05, 'epoch': 0.08}\n",
      "{'loss': 2.2031, 'learning_rate': 8.740000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.1562, 'learning_rate': 8.730000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.1406, 'learning_rate': 8.72e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0938, 'learning_rate': 8.71e-05, 'epoch': 0.08}\n",
      "{'loss': 2.1562, 'learning_rate': 8.7e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0469, 'learning_rate': 8.69e-05, 'epoch': 0.08}\n",
      "{'loss': 1.9766, 'learning_rate': 8.680000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0156, 'learning_rate': 8.67e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9609, 'learning_rate': 8.66e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9297, 'learning_rate': 8.65e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8672, 'learning_rate': 8.64e-05, 'epoch': 0.09}\n",
      "{'loss': 2.0781, 'learning_rate': 8.63e-05, 'epoch': 0.09}\n",
      "{'loss': 2.5781, 'learning_rate': 8.620000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8438, 'learning_rate': 8.61e-05, 'epoch': 0.09}\n",
      "{'loss': 2.1562, 'learning_rate': 8.6e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8984, 'learning_rate': 8.59e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9297, 'learning_rate': 8.58e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7656, 'learning_rate': 8.57e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7734, 'learning_rate': 8.560000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 1.75, 'learning_rate': 8.55e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7344, 'learning_rate': 8.54e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9219, 'learning_rate': 8.53e-05, 'epoch': 0.09}\n",
      "{'loss': 1.7891, 'learning_rate': 8.52e-05, 'epoch': 0.09}\n",
      "{'loss': 1.6484, 'learning_rate': 8.510000000000001e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6875, 'learning_rate': 8.5e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6172, 'learning_rate': 8.49e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6484, 'learning_rate': 8.48e-05, 'epoch': 0.1}\n",
      "{'loss': 1.5859, 'learning_rate': 8.47e-05, 'epoch': 0.1}\n",
      "{'loss': 2.0, 'learning_rate': 8.46e-05, 'epoch': 0.1}\n",
      "{'loss': 1.5781, 'learning_rate': 8.450000000000001e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7109, 'learning_rate': 8.44e-05, 'epoch': 0.1}\n",
      "{'loss': 2.0625, 'learning_rate': 8.43e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8203, 'learning_rate': 8.42e-05, 'epoch': 0.1}\n",
      "{'loss': 1.4453, 'learning_rate': 8.41e-05, 'epoch': 0.1}\n",
      "{'loss': 1.5156, 'learning_rate': 8.4e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6953, 'learning_rate': 8.39e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7266, 'learning_rate': 8.38e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7891, 'learning_rate': 8.37e-05, 'epoch': 0.1}\n",
      "{'loss': 1.4609, 'learning_rate': 8.36e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5312, 'learning_rate': 8.35e-05, 'epoch': 0.11}\n",
      "{'loss': 1.4766, 'learning_rate': 8.34e-05, 'epoch': 0.11}\n",
      "{'loss': 1.4766, 'learning_rate': 8.33e-05, 'epoch': 0.11}\n",
      "{'loss': 1.3594, 'learning_rate': 8.32e-05, 'epoch': 0.11}\n",
      "{'loss': 1.375, 'learning_rate': 8.31e-05, 'epoch': 0.11}\n",
      "{'loss': 1.3906, 'learning_rate': 8.3e-05, 'epoch': 0.11}\n",
      "{'loss': 1.4062, 'learning_rate': 8.29e-05, 'epoch': 0.11}\n",
      "{'loss': 1.3516, 'learning_rate': 8.28e-05, 'epoch': 0.11}\n",
      "{'loss': 1.7734, 'learning_rate': 8.27e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5469, 'learning_rate': 8.26e-05, 'epoch': 0.11}\n",
      "{'loss': 1.9219, 'learning_rate': 8.25e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5859, 'learning_rate': 8.24e-05, 'epoch': 0.11}\n",
      "{'loss': 1.4062, 'learning_rate': 8.23e-05, 'epoch': 0.11}\n",
      "{'loss': 1.2734, 'learning_rate': 8.22e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5703, 'learning_rate': 8.21e-05, 'epoch': 0.11}\n",
      "{'loss': 2.4688, 'learning_rate': 8.2e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2578, 'learning_rate': 8.19e-05, 'epoch': 0.12}\n",
      "{'loss': 1.3047, 'learning_rate': 8.18e-05, 'epoch': 0.12}\n",
      "{'loss': 1.1719, 'learning_rate': 8.17e-05, 'epoch': 0.12}\n",
      "{'loss': 1.25, 'learning_rate': 8.16e-05, 'epoch': 0.12}\n",
      "{'loss': 1.3125, 'learning_rate': 8.15e-05, 'epoch': 0.12}\n",
      "{'loss': 1.1797, 'learning_rate': 8.14e-05, 'epoch': 0.12}\n",
      "{'loss': 1.3125, 'learning_rate': 8.13e-05, 'epoch': 0.12}\n",
      "{'loss': 1.5625, 'learning_rate': 8.120000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.4375, 'learning_rate': 8.11e-05, 'epoch': 0.12}\n",
      "{'loss': 1.1719, 'learning_rate': 8.1e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2891, 'learning_rate': 8.090000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2109, 'learning_rate': 8.080000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.1875, 'learning_rate': 8.070000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.1953, 'learning_rate': 8.060000000000001e-05, 'epoch': 0.12}\n",
      "{'loss': 1.0469, 'learning_rate': 8.05e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0859, 'learning_rate': 8.04e-05, 'epoch': 0.13}\n",
      "{'loss': 1.125, 'learning_rate': 8.030000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.1172, 'learning_rate': 8.020000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0547, 'learning_rate': 8.010000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0938, 'learning_rate': 8e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0859, 'learning_rate': 7.99e-05, 'epoch': 0.13}\n",
      "{'loss': 1.1641, 'learning_rate': 7.98e-05, 'epoch': 0.13}\n",
      "{'loss': 1.2266, 'learning_rate': 7.970000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0469, 'learning_rate': 7.960000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0703, 'learning_rate': 7.950000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.3203, 'learning_rate': 7.94e-05, 'epoch': 0.13}\n",
      "{'loss': 0.9844, 'learning_rate': 7.93e-05, 'epoch': 0.13}\n",
      "{'loss': 1.3984, 'learning_rate': 7.920000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0078, 'learning_rate': 7.910000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 1.0156, 'learning_rate': 7.900000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 0.9492, 'learning_rate': 7.890000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9922, 'learning_rate': 7.88e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9805, 'learning_rate': 7.87e-05, 'epoch': 0.14}\n",
      "{'loss': 1.1172, 'learning_rate': 7.860000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9922, 'learning_rate': 7.850000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9219, 'learning_rate': 7.840000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 1.1094, 'learning_rate': 7.83e-05, 'epoch': 0.14}\n",
      "{'loss': 1.3906, 'learning_rate': 7.82e-05, 'epoch': 0.14}\n",
      "{'loss': 1.0859, 'learning_rate': 7.81e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9141, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8945, 'learning_rate': 7.790000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9258, 'learning_rate': 7.780000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8828, 'learning_rate': 7.77e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9766, 'learning_rate': 7.76e-05, 'epoch': 0.14}\n",
      "{'loss': 0.9219, 'learning_rate': 7.75e-05, 'epoch': 0.14}\n",
      "{'loss': 1.1406, 'learning_rate': 7.740000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 1.0781, 'learning_rate': 7.730000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 0.832, 'learning_rate': 7.72e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9062, 'learning_rate': 7.71e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9023, 'learning_rate': 7.7e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9375, 'learning_rate': 7.69e-05, 'epoch': 0.15}\n",
      "{'loss': 0.9062, 'learning_rate': 7.680000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 0.7891, 'learning_rate': 7.670000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 0.875, 'learning_rate': 7.66e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8203, 'learning_rate': 7.65e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8281, 'learning_rate': 7.64e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8047, 'learning_rate': 7.630000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 1.4766, 'learning_rate': 7.620000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 0.7969, 'learning_rate': 7.61e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8984, 'learning_rate': 7.6e-05, 'epoch': 0.15}\n",
      "{'loss': 0.7969, 'learning_rate': 7.59e-05, 'epoch': 0.15}\n",
      "{'loss': 0.8164, 'learning_rate': 7.58e-05, 'epoch': 0.16}\n",
      "{'loss': 0.9336, 'learning_rate': 7.570000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.8164, 'learning_rate': 7.560000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.957, 'learning_rate': 7.55e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7305, 'learning_rate': 7.54e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6406, 'learning_rate': 7.53e-05, 'epoch': 0.16}\n",
      "{'loss': 0.8633, 'learning_rate': 7.52e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7734, 'learning_rate': 7.510000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6992, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.832, 'learning_rate': 7.49e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4219, 'learning_rate': 7.48e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7539, 'learning_rate': 7.47e-05, 'epoch': 0.16}\n",
      "{'loss': 0.8633, 'learning_rate': 7.46e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7148, 'learning_rate': 7.450000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7227, 'learning_rate': 7.44e-05, 'epoch': 0.16}\n",
      "{'loss': 0.8086, 'learning_rate': 7.43e-05, 'epoch': 0.16}\n",
      "{'loss': 0.9727, 'learning_rate': 7.42e-05, 'epoch': 0.17}\n",
      "{'loss': 0.9609, 'learning_rate': 7.41e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7188, 'learning_rate': 7.4e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7539, 'learning_rate': 7.390000000000001e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7656, 'learning_rate': 7.38e-05, 'epoch': 0.17}\n",
      "{'loss': 0.9062, 'learning_rate': 7.37e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7031, 'learning_rate': 7.36e-05, 'epoch': 0.17}\n",
      "{'loss': 0.793, 'learning_rate': 7.35e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7227, 'learning_rate': 7.340000000000001e-05, 'epoch': 0.17}\n",
      "{'loss': 1.0, 'learning_rate': 7.33e-05, 'epoch': 0.17}\n",
      "{'loss': 0.6797, 'learning_rate': 7.32e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7852, 'learning_rate': 7.31e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7617, 'learning_rate': 7.3e-05, 'epoch': 0.17}\n",
      "{'loss': 0.6367, 'learning_rate': 7.29e-05, 'epoch': 0.17}\n",
      "{'loss': 0.707, 'learning_rate': 7.280000000000001e-05, 'epoch': 0.17}\n",
      "{'loss': 0.9062, 'learning_rate': 7.27e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6953, 'learning_rate': 7.26e-05, 'epoch': 0.18}\n",
      "{'loss': 0.7109, 'learning_rate': 7.25e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6602, 'learning_rate': 7.24e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6953, 'learning_rate': 7.23e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6406, 'learning_rate': 7.22e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6328, 'learning_rate': 7.21e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6992, 'learning_rate': 7.2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6484, 'learning_rate': 7.19e-05, 'epoch': 0.18}\n",
      "{'loss': 0.7617, 'learning_rate': 7.18e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6758, 'learning_rate': 7.17e-05, 'epoch': 0.18}\n",
      "{'loss': 0.9453, 'learning_rate': 7.16e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6094, 'learning_rate': 7.15e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6484, 'learning_rate': 7.14e-05, 'epoch': 0.18}\n",
      "{'loss': 0.5586, 'learning_rate': 7.13e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6719, 'learning_rate': 7.12e-05, 'epoch': 0.18}\n",
      "{'loss': 0.8203, 'learning_rate': 7.11e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6406, 'learning_rate': 7.1e-05, 'epoch': 0.19}\n",
      "{'loss': 0.7031, 'learning_rate': 7.09e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6406, 'learning_rate': 7.08e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6289, 'learning_rate': 7.07e-05, 'epoch': 0.19}\n",
      "{'loss': 0.625, 'learning_rate': 7.06e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6328, 'learning_rate': 7.05e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5508, 'learning_rate': 7.04e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6367, 'learning_rate': 7.03e-05, 'epoch': 0.19}\n",
      "{'loss': 0.9414, 'learning_rate': 7.02e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5742, 'learning_rate': 7.01e-05, 'epoch': 0.19}\n",
      "{'loss': 0.668, 'learning_rate': 7e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6133, 'learning_rate': 6.99e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5117, 'learning_rate': 6.98e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5547, 'learning_rate': 6.97e-05, 'epoch': 0.19}\n",
      "{'loss': 0.6055, 'learning_rate': 6.96e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5781, 'learning_rate': 6.95e-05, 'epoch': 0.2}\n",
      "{'loss': 0.7188, 'learning_rate': 6.939999999999999e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6289, 'learning_rate': 6.93e-05, 'epoch': 0.2}\n",
      "{'loss': 0.8398, 'learning_rate': 6.92e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6055, 'learning_rate': 6.91e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5, 'learning_rate': 6.9e-05, 'epoch': 0.2}\n",
      "{'loss': 0.7734, 'learning_rate': 6.89e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5703, 'learning_rate': 6.879999999999999e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5742, 'learning_rate': 6.87e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5859, 'learning_rate': 6.860000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5859, 'learning_rate': 6.850000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6172, 'learning_rate': 6.840000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5508, 'learning_rate': 6.83e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5625, 'learning_rate': 6.82e-05, 'epoch': 0.2}\n",
      "{'loss': 0.8203, 'learning_rate': 6.81e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5352, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5469, 'learning_rate': 6.790000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5078, 'learning_rate': 6.780000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5898, 'learning_rate': 6.77e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5703, 'learning_rate': 6.76e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5117, 'learning_rate': 6.750000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.8047, 'learning_rate': 6.740000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4707, 'learning_rate': 6.730000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5898, 'learning_rate': 6.720000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.918, 'learning_rate': 6.71e-05, 'epoch': 0.21}\n",
      "{'loss': 0.7852, 'learning_rate': 6.7e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4961, 'learning_rate': 6.690000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4863, 'learning_rate': 6.680000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4707, 'learning_rate': 6.670000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4883, 'learning_rate': 6.66e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5352, 'learning_rate': 6.65e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5078, 'learning_rate': 6.64e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5156, 'learning_rate': 6.630000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5078, 'learning_rate': 6.620000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.6406, 'learning_rate': 6.610000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4883, 'learning_rate': 6.6e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7227, 'learning_rate': 6.59e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4766, 'learning_rate': 6.58e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4941, 'learning_rate': 6.570000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.6953, 'learning_rate': 6.560000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4961, 'learning_rate': 6.55e-05, 'epoch': 0.22}\n",
      "{'loss': 0.7109, 'learning_rate': 6.54e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4805, 'learning_rate': 6.53e-05, 'epoch': 0.22}\n",
      "{'loss': 0.582, 'learning_rate': 6.52e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4707, 'learning_rate': 6.510000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4746, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4707, 'learning_rate': 6.49e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4531, 'learning_rate': 6.48e-05, 'epoch': 0.23}\n",
      "{'loss': 0.8438, 'learning_rate': 6.47e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5586, 'learning_rate': 6.460000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4902, 'learning_rate': 6.450000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5273, 'learning_rate': 6.440000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4648, 'learning_rate': 6.43e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4785, 'learning_rate': 6.42e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4902, 'learning_rate': 6.41e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4688, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4902, 'learning_rate': 6.390000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4941, 'learning_rate': 6.38e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4609, 'learning_rate': 6.37e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5312, 'learning_rate': 6.36e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4355, 'learning_rate': 6.35e-05, 'epoch': 0.23}\n",
      "{'loss': 0.5117, 'learning_rate': 6.340000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4609, 'learning_rate': 6.330000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 0.459, 'learning_rate': 6.32e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4258, 'learning_rate': 6.31e-05, 'epoch': 0.24}\n",
      "{'loss': 0.9219, 'learning_rate': 6.3e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4863, 'learning_rate': 6.29e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4922, 'learning_rate': 6.280000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 0.6758, 'learning_rate': 6.27e-05, 'epoch': 0.24}\n",
      "{'loss': 0.7227, 'learning_rate': 6.26e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4258, 'learning_rate': 6.25e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3984, 'learning_rate': 6.24e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4473, 'learning_rate': 6.23e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4629, 'learning_rate': 6.220000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4453, 'learning_rate': 6.21e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4062, 'learning_rate': 6.2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4883, 'learning_rate': 6.19e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3926, 'learning_rate': 6.18e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5938, 'learning_rate': 6.170000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4102, 'learning_rate': 6.16e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4648, 'learning_rate': 6.15e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4336, 'learning_rate': 6.14e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4941, 'learning_rate': 6.13e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4746, 'learning_rate': 6.12e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4121, 'learning_rate': 6.110000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4043, 'learning_rate': 6.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4277, 'learning_rate': 6.09e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4141, 'learning_rate': 6.08e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4316, 'learning_rate': 6.07e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4199, 'learning_rate': 6.06e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4805, 'learning_rate': 6.05e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7305, 'learning_rate': 6.04e-05, 'epoch': 0.25}\n",
      "{'loss': 0.5859, 'learning_rate': 6.03e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4141, 'learning_rate': 6.02e-05, 'epoch': 0.26}\n",
      "{'loss': 0.582, 'learning_rate': 6.0100000000000004e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4023, 'learning_rate': 6e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3984, 'learning_rate': 5.99e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4883, 'learning_rate': 5.9800000000000003e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4219, 'learning_rate': 5.97e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4492, 'learning_rate': 5.96e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4238, 'learning_rate': 5.95e-05, 'epoch': 0.26}\n",
      "{'loss': 0.582, 'learning_rate': 5.94e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4434, 'learning_rate': 5.93e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3945, 'learning_rate': 5.92e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4629, 'learning_rate': 5.91e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4141, 'learning_rate': 5.9e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4258, 'learning_rate': 5.89e-05, 'epoch': 0.26}\n",
      "{'loss': 0.373, 'learning_rate': 5.88e-05, 'epoch': 0.26}\n",
      "{'loss': 0.7305, 'learning_rate': 5.87e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4551, 'learning_rate': 5.86e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3926, 'learning_rate': 5.85e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3945, 'learning_rate': 5.8399999999999997e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3809, 'learning_rate': 5.83e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3789, 'learning_rate': 5.82e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4414, 'learning_rate': 5.8099999999999996e-05, 'epoch': 0.27}\n",
      "{'loss': 0.375, 'learning_rate': 5.8e-05, 'epoch': 0.27}\n",
      "{'loss': 0.7383, 'learning_rate': 5.79e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4375, 'learning_rate': 5.7799999999999995e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4277, 'learning_rate': 5.77e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3887, 'learning_rate': 5.76e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4375, 'learning_rate': 5.7499999999999995e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4277, 'learning_rate': 5.74e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6758, 'learning_rate': 5.73e-05, 'epoch': 0.27}\n",
      "{'loss': 0.5664, 'learning_rate': 5.72e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4238, 'learning_rate': 5.71e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4141, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5234, 'learning_rate': 5.69e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3867, 'learning_rate': 5.68e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3926, 'learning_rate': 5.6699999999999996e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3867, 'learning_rate': 5.66e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5352, 'learning_rate': 5.65e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3926, 'learning_rate': 5.6399999999999995e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3867, 'learning_rate': 5.63e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3691, 'learning_rate': 5.620000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4121, 'learning_rate': 5.610000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6094, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4277, 'learning_rate': 5.590000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.375, 'learning_rate': 5.580000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4336, 'learning_rate': 5.5700000000000005e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3613, 'learning_rate': 5.560000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4531, 'learning_rate': 5.550000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 1.4766, 'learning_rate': 5.5400000000000005e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4746, 'learning_rate': 5.530000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4434, 'learning_rate': 5.520000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3574, 'learning_rate': 5.5100000000000004e-05, 'epoch': 0.29}\n",
      "{'loss': 0.457, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3906, 'learning_rate': 5.4900000000000006e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3457, 'learning_rate': 5.4800000000000004e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5469, 'learning_rate': 5.470000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5586, 'learning_rate': 5.4600000000000006e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3984, 'learning_rate': 5.45e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5273, 'learning_rate': 5.440000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3594, 'learning_rate': 5.4300000000000005e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3828, 'learning_rate': 5.420000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3281, 'learning_rate': 5.410000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4551, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3906, 'learning_rate': 5.390000000000001e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3555, 'learning_rate': 5.380000000000001e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3789, 'learning_rate': 5.3700000000000004e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3535, 'learning_rate': 5.360000000000001e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3691, 'learning_rate': 5.3500000000000006e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3711, 'learning_rate': 5.3400000000000004e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3926, 'learning_rate': 5.330000000000001e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3613, 'learning_rate': 5.3200000000000006e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3789, 'learning_rate': 5.31e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3809, 'learning_rate': 5.300000000000001e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4121, 'learning_rate': 5.2900000000000005e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6016, 'learning_rate': 5.28e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3965, 'learning_rate': 5.270000000000001e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4727, 'learning_rate': 5.2600000000000005e-05, 'epoch': 0.3}\n",
      "{'loss': 0.5469, 'learning_rate': 5.25e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3379, 'learning_rate': 5.2400000000000007e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3379, 'learning_rate': 5.2300000000000004e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3809, 'learning_rate': 5.22e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3984, 'learning_rate': 5.2100000000000006e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3887, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4199, 'learning_rate': 5.19e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4258, 'learning_rate': 5.1800000000000005e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3965, 'learning_rate': 5.17e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3867, 'learning_rate': 5.16e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3223, 'learning_rate': 5.1500000000000005e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5547, 'learning_rate': 5.14e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4473, 'learning_rate': 5.130000000000001e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3945, 'learning_rate': 5.1200000000000004e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5039, 'learning_rate': 5.11e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4219, 'learning_rate': 5.1000000000000006e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3223, 'learning_rate': 5.0900000000000004e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3496, 'learning_rate': 5.08e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3301, 'learning_rate': 5.0700000000000006e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3867, 'learning_rate': 5.0600000000000003e-05, 'epoch': 0.32}\n",
      "{'loss': 0.6875, 'learning_rate': 5.05e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4922, 'learning_rate': 5.0400000000000005e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3496, 'learning_rate': 5.03e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4219, 'learning_rate': 5.02e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5703, 'learning_rate': 5.0100000000000005e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3711, 'learning_rate': 5e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5117, 'learning_rate': 4.99e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3809, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4023, 'learning_rate': 4.97e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3535, 'learning_rate': 4.96e-05, 'epoch': 0.32}\n",
      "{'loss': 0.377, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3516, 'learning_rate': 4.94e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3711, 'learning_rate': 4.93e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3965, 'learning_rate': 4.92e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3652, 'learning_rate': 4.91e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4121, 'learning_rate': 4.9e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3516, 'learning_rate': 4.89e-05, 'epoch': 0.33}\n",
      "{'loss': 0.377, 'learning_rate': 4.88e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3828, 'learning_rate': 4.87e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3457, 'learning_rate': 4.86e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3438, 'learning_rate': 4.85e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3613, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4707, 'learning_rate': 4.83e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3984, 'learning_rate': 4.82e-05, 'epoch': 0.33}\n",
      "{'loss': 0.377, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3535, 'learning_rate': 4.8e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3652, 'learning_rate': 4.79e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3789, 'learning_rate': 4.78e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3359, 'learning_rate': 4.77e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5977, 'learning_rate': 4.76e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3516, 'learning_rate': 4.75e-05, 'epoch': 0.34}\n",
      "{'loss': 0.8398, 'learning_rate': 4.74e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3418, 'learning_rate': 4.73e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3438, 'learning_rate': 4.72e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3008, 'learning_rate': 4.71e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3477, 'learning_rate': 4.7e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3613, 'learning_rate': 4.69e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3594, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3359, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3223, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3223, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3262, 'learning_rate': 4.64e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3984, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3105, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 0.334, 'learning_rate': 4.61e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3789, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3926, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3184, 'learning_rate': 4.58e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3047, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2949, 'learning_rate': 4.55e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4805, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3418, 'learning_rate': 4.53e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5508, 'learning_rate': 4.52e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3047, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3066, 'learning_rate': 4.5e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3223, 'learning_rate': 4.49e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3242, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3281, 'learning_rate': 4.47e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3262, 'learning_rate': 4.46e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3691, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3359, 'learning_rate': 4.44e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3477, 'learning_rate': 4.43e-05, 'epoch': 0.36}\n",
      "{'loss': 0.375, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6953, 'learning_rate': 4.41e-05, 'epoch': 0.36}\n",
      "{'loss': 0.373, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3301, 'learning_rate': 4.39e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3672, 'learning_rate': 4.38e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7188, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6953, 'learning_rate': 4.36e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3594, 'learning_rate': 4.35e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3555, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.36}\n",
      "{'loss': 0.5117, 'learning_rate': 4.33e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3906, 'learning_rate': 4.32e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3516, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3809, 'learning_rate': 4.3e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3223, 'learning_rate': 4.29e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4395, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2949, 'learning_rate': 4.27e-05, 'epoch': 0.37}\n",
      "{'loss': 0.373, 'learning_rate': 4.26e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3008, 'learning_rate': 4.25e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3262, 'learning_rate': 4.24e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3047, 'learning_rate': 4.23e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3281, 'learning_rate': 4.22e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3984, 'learning_rate': 4.21e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2793, 'learning_rate': 4.2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.668, 'learning_rate': 4.19e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3086, 'learning_rate': 4.18e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3027, 'learning_rate': 4.17e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4551, 'learning_rate': 4.16e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3672, 'learning_rate': 4.15e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3516, 'learning_rate': 4.14e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5, 'learning_rate': 4.13e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3594, 'learning_rate': 4.12e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3105, 'learning_rate': 4.11e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3047, 'learning_rate': 4.1e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3418, 'learning_rate': 4.09e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4766, 'learning_rate': 4.08e-05, 'epoch': 0.38}\n",
      "{'loss': 0.293, 'learning_rate': 4.07e-05, 'epoch': 0.38}\n",
      "{'loss': 0.332, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3086, 'learning_rate': 4.05e-05, 'epoch': 0.38}\n",
      "{'loss': 0.293, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3418, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.38}\n",
      "{'loss': 0.334, 'learning_rate': 4.02e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3203, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3145, 'learning_rate': 4e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3418, 'learning_rate': 3.99e-05, 'epoch': 0.39}\n",
      "{'loss': 0.377, 'learning_rate': 3.9800000000000005e-05, 'epoch': 0.39}\n",
      "{'loss': 0.334, 'learning_rate': 3.97e-05, 'epoch': 0.39}\n",
      "{'loss': 0.7812, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4629, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3379, 'learning_rate': 3.94e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3066, 'learning_rate': 3.9300000000000007e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3066, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3027, 'learning_rate': 3.91e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3242, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3672, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.39}\n",
      "{'loss': 0.5469, 'learning_rate': 3.88e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3457, 'learning_rate': 3.8700000000000006e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3574, 'learning_rate': 3.86e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3574, 'learning_rate': 3.85e-05, 'epoch': 0.39}\n",
      "{'loss': 0.291, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.4}\n",
      "{'loss': 0.375, 'learning_rate': 3.83e-05, 'epoch': 0.4}\n",
      "{'loss': 1.0078, 'learning_rate': 3.82e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3379, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3047, 'learning_rate': 3.8e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3184, 'learning_rate': 3.79e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4238, 'learning_rate': 3.7800000000000004e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3125, 'learning_rate': 3.77e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3125, 'learning_rate': 3.76e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3008, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5117, 'learning_rate': 3.74e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3105, 'learning_rate': 3.73e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4023, 'learning_rate': 3.72e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2715, 'learning_rate': 3.71e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3281, 'learning_rate': 3.7e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4688, 'learning_rate': 3.69e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3789, 'learning_rate': 3.68e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3301, 'learning_rate': 3.6700000000000004e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3145, 'learning_rate': 3.66e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3594, 'learning_rate': 3.65e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3887, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3223, 'learning_rate': 3.63e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3789, 'learning_rate': 3.62e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3906, 'learning_rate': 3.61e-05, 'epoch': 0.41}\n",
      "{'loss': 0.4668, 'learning_rate': 3.6e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3301, 'learning_rate': 3.59e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3086, 'learning_rate': 3.58e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2754, 'learning_rate': 3.57e-05, 'epoch': 0.41}\n",
      "{'loss': 0.5195, 'learning_rate': 3.56e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3496, 'learning_rate': 3.55e-05, 'epoch': 0.41}\n",
      "{'loss': 0.291, 'learning_rate': 3.54e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3027, 'learning_rate': 3.53e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3594, 'learning_rate': 3.52e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3359, 'learning_rate': 3.51e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2891, 'learning_rate': 3.5e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3613, 'learning_rate': 3.49e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3379, 'learning_rate': 3.48e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3359, 'learning_rate': 3.4699999999999996e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3887, 'learning_rate': 3.46e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3457, 'learning_rate': 3.45e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3105, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3672, 'learning_rate': 3.430000000000001e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3008, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3652, 'learning_rate': 3.41e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5039, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.42}\n",
      "{'loss': 0.375, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.42}\n",
      "{'loss': 0.373, 'learning_rate': 3.38e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2891, 'learning_rate': 3.3700000000000006e-05, 'epoch': 0.43}\n",
      "{'loss': 0.6602, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2969, 'learning_rate': 3.35e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3066, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3105, 'learning_rate': 3.33e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3184, 'learning_rate': 3.32e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3086, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3086, 'learning_rate': 3.3e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5898, 'learning_rate': 3.29e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4316, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4258, 'learning_rate': 3.27e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2852, 'learning_rate': 3.26e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3027, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3477, 'learning_rate': 3.24e-05, 'epoch': 0.43}\n",
      "{'loss': 0.6055, 'learning_rate': 3.2300000000000006e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4922, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3164, 'learning_rate': 3.21e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3027, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3516, 'learning_rate': 3.19e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3535, 'learning_rate': 3.18e-05, 'epoch': 0.44}\n",
      "{'loss': 0.4316, 'learning_rate': 3.1700000000000005e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3027, 'learning_rate': 3.16e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3203, 'learning_rate': 3.15e-05, 'epoch': 0.44}\n",
      "{'loss': 0.293, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3047, 'learning_rate': 3.13e-05, 'epoch': 0.44}\n",
      "{'loss': 0.5078, 'learning_rate': 3.12e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3945, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2949, 'learning_rate': 3.1e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3027, 'learning_rate': 3.09e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3184, 'learning_rate': 3.08e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3418, 'learning_rate': 3.07e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2676, 'learning_rate': 3.06e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3242, 'learning_rate': 3.05e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3359, 'learning_rate': 3.04e-05, 'epoch': 0.45}\n",
      "{'loss': 0.334, 'learning_rate': 3.03e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2969, 'learning_rate': 3.02e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4824, 'learning_rate': 3.01e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3086, 'learning_rate': 3e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3457, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3535, 'learning_rate': 2.98e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2969, 'learning_rate': 2.97e-05, 'epoch': 0.45}\n",
      "{'loss': 0.252, 'learning_rate': 2.96e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2832, 'learning_rate': 2.95e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4355, 'learning_rate': 2.94e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4629, 'learning_rate': 2.93e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2988, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3047, 'learning_rate': 2.91e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2852, 'learning_rate': 2.9e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2949, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.46}\n",
      "{'loss': 0.5664, 'learning_rate': 2.88e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3242, 'learning_rate': 2.87e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3105, 'learning_rate': 2.86e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2793, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2871, 'learning_rate': 2.84e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3242, 'learning_rate': 2.83e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3516, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3555, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3262, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3809, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3496, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2715, 'learning_rate': 2.7700000000000002e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3574, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3379, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3047, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.4004, 'learning_rate': 2.7300000000000003e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3594, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 0.4961, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.47}\n",
      "{'loss': 0.4277, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3027, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3066, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3203, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.334, 'learning_rate': 2.6600000000000003e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3145, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3047, 'learning_rate': 2.64e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3203, 'learning_rate': 2.6300000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3398, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3184, 'learning_rate': 2.61e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3164, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3262, 'learning_rate': 2.5900000000000003e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3047, 'learning_rate': 2.58e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3203, 'learning_rate': 2.57e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5156, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2754, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2734, 'learning_rate': 2.54e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2852, 'learning_rate': 2.5300000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2793, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4277, 'learning_rate': 2.51e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3047, 'learning_rate': 2.5e-05, 'epoch': 0.48}\n",
      "{'loss': 0.293, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3457, 'learning_rate': 2.48e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2949, 'learning_rate': 2.47e-05, 'epoch': 0.48}\n",
      "{'loss': 0.332, 'learning_rate': 2.46e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3516, 'learning_rate': 2.45e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3516, 'learning_rate': 2.44e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3262, 'learning_rate': 2.43e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2832, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3516, 'learning_rate': 2.41e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3008, 'learning_rate': 2.4e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2852, 'learning_rate': 2.39e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3301, 'learning_rate': 2.38e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6445, 'learning_rate': 2.37e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4355, 'learning_rate': 2.36e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3242, 'learning_rate': 2.35e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3008, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.49}\n",
      "{'loss': 0.293, 'learning_rate': 2.3300000000000004e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2773, 'learning_rate': 2.32e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3164, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3379, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3359, 'learning_rate': 2.29e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2559, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3594, 'learning_rate': 2.2700000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2676, 'learning_rate': 2.26e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4961, 'learning_rate': 2.25e-05, 'epoch': 0.5}\n",
      "{'loss': 0.293, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.291, 'learning_rate': 2.23e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3496, 'learning_rate': 2.22e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2656, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3066, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3086, 'learning_rate': 2.19e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5352, 'learning_rate': 2.18e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3301, 'learning_rate': 2.1700000000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.498, 'learning_rate': 2.16e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3008, 'learning_rate': 2.15e-05, 'epoch': 0.5}\n",
      "{'loss': 0.291, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3281, 'learning_rate': 2.13e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3223, 'learning_rate': 2.12e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3242, 'learning_rate': 2.11e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2754, 'learning_rate': 2.1e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2871, 'learning_rate': 2.09e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3086, 'learning_rate': 2.08e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3086, 'learning_rate': 2.07e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3418, 'learning_rate': 2.06e-05, 'epoch': 0.51}\n",
      "{'loss': 0.4375, 'learning_rate': 2.05e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3047, 'learning_rate': 2.04e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3281, 'learning_rate': 2.0300000000000002e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3711, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2754, 'learning_rate': 2.01e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2969, 'learning_rate': 2e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3555, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.51}\n",
      "{'loss': 0.291, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.51}\n",
      "{'loss': 0.416, 'learning_rate': 1.97e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3066, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.52}\n",
      "{'loss': 0.252, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3418, 'learning_rate': 1.94e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2988, 'learning_rate': 1.93e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4707, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2676, 'learning_rate': 1.91e-05, 'epoch': 0.52}\n",
      "{'loss': 0.291, 'learning_rate': 1.9e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3535, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4434, 'learning_rate': 1.88e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4707, 'learning_rate': 1.87e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3027, 'learning_rate': 1.86e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2988, 'learning_rate': 1.85e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3262, 'learning_rate': 1.84e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4297, 'learning_rate': 1.83e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3125, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2949, 'learning_rate': 1.81e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3418, 'learning_rate': 1.8e-05, 'epoch': 0.53}\n",
      "{'loss': 0.377, 'learning_rate': 1.79e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3145, 'learning_rate': 1.78e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3164, 'learning_rate': 1.77e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2598, 'learning_rate': 1.76e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3477, 'learning_rate': 1.75e-05, 'epoch': 0.53}\n",
      "{'loss': 0.4121, 'learning_rate': 1.74e-05, 'epoch': 0.53}\n",
      "{'loss': 0.332, 'learning_rate': 1.73e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3496, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3379, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.291, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3047, 'learning_rate': 1.69e-05, 'epoch': 0.53}\n",
      "{'loss': 0.291, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3477, 'learning_rate': 1.6700000000000003e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2871, 'learning_rate': 1.66e-05, 'epoch': 0.54}\n",
      "{'loss': 0.4824, 'learning_rate': 1.65e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3398, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3008, 'learning_rate': 1.63e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3242, 'learning_rate': 1.62e-05, 'epoch': 0.54}\n",
      "{'loss': 0.291, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3086, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6133, 'learning_rate': 1.59e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2852, 'learning_rate': 1.58e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2734, 'learning_rate': 1.5700000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3652, 'learning_rate': 1.56e-05, 'epoch': 0.54}\n",
      "{'loss': 0.291, 'learning_rate': 1.55e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2656, 'learning_rate': 1.54e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2852, 'learning_rate': 1.53e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3281, 'learning_rate': 1.52e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2969, 'learning_rate': 1.51e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3223, 'learning_rate': 1.5e-05, 'epoch': 0.55}\n",
      "{'loss': 0.332, 'learning_rate': 1.49e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6172, 'learning_rate': 1.48e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3008, 'learning_rate': 1.47e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3066, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2871, 'learning_rate': 1.45e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3477, 'learning_rate': 1.44e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2773, 'learning_rate': 1.43e-05, 'epoch': 0.55}\n",
      "{'loss': 0.291, 'learning_rate': 1.42e-05, 'epoch': 0.55}\n",
      "{'loss': 0.293, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3203, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.55}\n",
      "{'loss': 0.332, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3184, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3086, 'learning_rate': 1.3700000000000001e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3008, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5156, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2891, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 0.291, 'learning_rate': 1.3300000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2832, 'learning_rate': 1.32e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2812, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3398, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3223, 'learning_rate': 1.29e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4453, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4785, 'learning_rate': 1.27e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4883, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3242, 'learning_rate': 1.25e-05, 'epoch': 0.56}\n",
      "{'loss': 0.293, 'learning_rate': 1.24e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2676, 'learning_rate': 1.23e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3125, 'learning_rate': 1.22e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3711, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3008, 'learning_rate': 1.2e-05, 'epoch': 0.56}\n",
      "{'loss': 0.332, 'learning_rate': 1.19e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5312, 'learning_rate': 1.18e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5781, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2559, 'learning_rate': 1.16e-05, 'epoch': 0.57}\n",
      "{'loss': 0.4492, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2852, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6953, 'learning_rate': 1.13e-05, 'epoch': 0.57}\n",
      "{'loss': 0.332, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2812, 'learning_rate': 1.11e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5469, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3242, 'learning_rate': 1.09e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3184, 'learning_rate': 1.08e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3066, 'learning_rate': 1.0700000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3145, 'learning_rate': 1.06e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2988, 'learning_rate': 1.05e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3203, 'learning_rate': 1.04e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3125, 'learning_rate': 1.03e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4453, 'learning_rate': 1.02e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3086, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.58}\n",
      "{'loss': 0.332, 'learning_rate': 1e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3184, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.58}\n",
      "{'loss': 0.2676, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3027, 'learning_rate': 9.7e-06, 'epoch': 0.58}\n",
      "{'loss': 0.2891, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3086, 'learning_rate': 9.5e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3145, 'learning_rate': 9.4e-06, 'epoch': 0.58}\n",
      "{'loss': 0.4824, 'learning_rate': 9.3e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3887, 'learning_rate': 9.2e-06, 'epoch': 0.58}\n",
      "{'loss': 0.291, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3203, 'learning_rate': 9e-06, 'epoch': 0.58}\n",
      "{'loss': 0.2734, 'learning_rate': 8.9e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3574, 'learning_rate': 8.8e-06, 'epoch': 0.59}\n",
      "{'loss': 0.332, 'learning_rate': 8.7e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3164, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3027, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.59}\n",
      "{'loss': 0.4902, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.59}\n",
      "{'loss': 0.2852, 'learning_rate': 8.3e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3086, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3496, 'learning_rate': 8.1e-06, 'epoch': 0.59}\n",
      "{'loss': 0.2754, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.59}\n",
      "{'loss': 0.2754, 'learning_rate': 7.9e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3203, 'learning_rate': 7.8e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3105, 'learning_rate': 7.7e-06, 'epoch': 0.59}\n",
      "{'loss': 0.334, 'learning_rate': 7.6e-06, 'epoch': 0.59}\n",
      "{'loss': 0.252, 'learning_rate': 7.5e-06, 'epoch': 0.59}\n",
      "{'loss': 0.334, 'learning_rate': 7.4e-06, 'epoch': 0.59}\n",
      "{'loss': 0.3047, 'learning_rate': 7.2999999999999996e-06, 'epoch': 0.59}\n",
      "{'loss': 0.4102, 'learning_rate': 7.2e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3066, 'learning_rate': 7.1e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3145, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.291, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3301, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.2676, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.332, 'learning_rate': 6.6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.2812, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3027, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.6}\n",
      "{'loss': 0.291, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.5977, 'learning_rate': 6.2e-06, 'epoch': 0.6}\n",
      "{'loss': 0.2969, 'learning_rate': 6.1e-06, 'epoch': 0.6}\n",
      "{'loss': 0.2676, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.2871, 'learning_rate': 5.9e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3203, 'learning_rate': 5.8e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3027, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3359, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.61}\n",
      "{'loss': 0.5547, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3359, 'learning_rate': 5.4e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3086, 'learning_rate': 5.3e-06, 'epoch': 0.61}\n",
      "{'loss': 0.2656, 'learning_rate': 5.2e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3418, 'learning_rate': 5.1e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3164, 'learning_rate': 5e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3164, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3105, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3789, 'learning_rate': 4.7e-06, 'epoch': 0.61}\n",
      "{'loss': 0.4648, 'learning_rate': 4.6e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3242, 'learning_rate': 4.5e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3047, 'learning_rate': 4.4e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3223, 'learning_rate': 4.2999999999999995e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3184, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.61}\n",
      "{'loss': 0.3457, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.62}\n",
      "{'loss': 0.4414, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3184, 'learning_rate': 3.9e-06, 'epoch': 0.62}\n",
      "{'loss': 0.2715, 'learning_rate': 3.8e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3516, 'learning_rate': 3.7e-06, 'epoch': 0.62}\n",
      "{'loss': 0.2754, 'learning_rate': 3.6e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3145, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.62}\n",
      "{'loss': 0.668, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3477, 'learning_rate': 3.3e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3184, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3086, 'learning_rate': 3.1e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3066, 'learning_rate': 3e-06, 'epoch': 0.62}\n",
      "{'loss': 0.2988, 'learning_rate': 2.9e-06, 'epoch': 0.62}\n",
      "{'loss': 0.2793, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3379, 'learning_rate': 2.7e-06, 'epoch': 0.62}\n",
      "{'loss': 0.2773, 'learning_rate': 2.6e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3047, 'learning_rate': 2.5e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3359, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.63}\n",
      "{'loss': 0.291, 'learning_rate': 2.3e-06, 'epoch': 0.63}\n",
      "{'loss': 0.2793, 'learning_rate': 2.2e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3574, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3086, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.63}\n",
      "{'loss': 0.293, 'learning_rate': 1.9e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3438, 'learning_rate': 1.8e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3438, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3145, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.63}\n",
      "{'loss': 0.2773, 'learning_rate': 1.5e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3184, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.63}\n",
      "{'loss': 0.2754, 'learning_rate': 1.3e-06, 'epoch': 0.63}\n",
      "{'loss': 0.2754, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.63}\n",
      "{'loss': 0.293, 'learning_rate': 1.1e-06, 'epoch': 0.63}\n",
      "{'loss': 0.3008, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.64}\n",
      "{'loss': 0.2988, 'learning_rate': 9e-07, 'epoch': 0.64}\n",
      "{'loss': 0.5, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.64}\n",
      "{'loss': 0.3203, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.64}\n",
      "{'loss': 0.5156, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.64}\n",
      "{'loss': 0.4004, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.64}\n",
      "{'loss': 0.3047, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.64}\n",
      "{'loss': 0.4219, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.64}\n",
      "{'loss': 0.3379, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.64}\n",
      "{'loss': 0.3457, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.64}\n",
      "{'loss': 0.3184, 'learning_rate': 0.0, 'epoch': 0.64}\n",
      "{'train_runtime': 227.3612, 'train_samples_per_second': 35.186, 'train_steps_per_second': 4.398, 'train_loss': 1.57051953125, 'epoch': 0.64}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.57051953125, metrics={'train_runtime': 227.3612, 'train_samples_per_second': 35.186, 'train_steps_per_second': 4.398, 'train_loss': 1.57051953125, 'epoch': 0.64})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "instruct_model = trainer.model\n",
    "original_model = original_model.to(device)\n",
    "instruct_model = instruct_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 7077888\n",
      "all model parameters: 254655744\n",
      "percentage of trainable model parameters: 2.78%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1,\n",
    "    max_steps=400   \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb73956ace945fcadbe771e51f7dbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3027, 'learning_rate': 0.0009975000000000001, 'epoch': 0.0}\n",
      "{'loss': 0.3262, 'learning_rate': 0.000995, 'epoch': 0.0}\n",
      "{'loss': 0.3535, 'learning_rate': 0.0009925000000000001, 'epoch': 0.0}\n",
      "{'loss': 0.3066, 'learning_rate': 0.00099, 'epoch': 0.0}\n",
      "{'loss': 0.291, 'learning_rate': 0.0009875, 'epoch': 0.0}\n",
      "{'loss': 0.3105, 'learning_rate': 0.000985, 'epoch': 0.0}\n",
      "{'loss': 0.5312, 'learning_rate': 0.0009825, 'epoch': 0.0}\n",
      "{'loss': 0.3359, 'learning_rate': 0.00098, 'epoch': 0.01}\n",
      "{'loss': 0.3223, 'learning_rate': 0.0009775, 'epoch': 0.01}\n",
      "{'loss': 0.2578, 'learning_rate': 0.000975, 'epoch': 0.01}\n",
      "{'loss': 0.3652, 'learning_rate': 0.0009725000000000001, 'epoch': 0.01}\n",
      "{'loss': 0.3184, 'learning_rate': 0.0009699999999999999, 'epoch': 0.01}\n",
      "{'loss': 0.2871, 'learning_rate': 0.0009675, 'epoch': 0.01}\n",
      "{'loss': 0.2734, 'learning_rate': 0.000965, 'epoch': 0.01}\n",
      "{'loss': 0.2969, 'learning_rate': 0.0009625, 'epoch': 0.01}\n",
      "{'loss': 0.2969, 'learning_rate': 0.00096, 'epoch': 0.01}\n",
      "{'loss': 0.2754, 'learning_rate': 0.0009575, 'epoch': 0.01}\n",
      "{'loss': 0.4668, 'learning_rate': 0.000955, 'epoch': 0.01}\n",
      "{'loss': 0.3926, 'learning_rate': 0.0009525, 'epoch': 0.01}\n",
      "{'loss': 0.4434, 'learning_rate': 0.00095, 'epoch': 0.01}\n",
      "{'loss': 0.2422, 'learning_rate': 0.0009475, 'epoch': 0.01}\n",
      "{'loss': 0.2402, 'learning_rate': 0.000945, 'epoch': 0.01}\n",
      "{'loss': 0.2852, 'learning_rate': 0.0009425, 'epoch': 0.01}\n",
      "{'loss': 0.2031, 'learning_rate': 0.00094, 'epoch': 0.02}\n",
      "{'loss': 0.2275, 'learning_rate': 0.0009375, 'epoch': 0.02}\n",
      "{'loss': 0.2344, 'learning_rate': 0.0009350000000000001, 'epoch': 0.02}\n",
      "{'loss': 0.3027, 'learning_rate': 0.0009325000000000001, 'epoch': 0.02}\n",
      "{'loss': 0.1865, 'learning_rate': 0.00093, 'epoch': 0.02}\n",
      "{'loss': 0.2021, 'learning_rate': 0.0009275, 'epoch': 0.02}\n",
      "{'loss': 0.2969, 'learning_rate': 0.000925, 'epoch': 0.02}\n",
      "{'loss': 0.208, 'learning_rate': 0.0009225, 'epoch': 0.02}\n",
      "{'loss': 0.207, 'learning_rate': 0.00092, 'epoch': 0.02}\n",
      "{'loss': 0.2988, 'learning_rate': 0.0009175, 'epoch': 0.02}\n",
      "{'loss': 0.1982, 'learning_rate': 0.000915, 'epoch': 0.02}\n",
      "{'loss': 0.3574, 'learning_rate': 0.0009125, 'epoch': 0.02}\n",
      "{'loss': 0.1602, 'learning_rate': 0.00091, 'epoch': 0.02}\n",
      "{'loss': 0.1641, 'learning_rate': 0.0009075, 'epoch': 0.02}\n",
      "{'loss': 0.1934, 'learning_rate': 0.0009050000000000001, 'epoch': 0.02}\n",
      "{'loss': 0.1787, 'learning_rate': 0.0009025, 'epoch': 0.03}\n",
      "{'loss': 0.1514, 'learning_rate': 0.0009000000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.1768, 'learning_rate': 0.0008975, 'epoch': 0.03}\n",
      "{'loss': 0.1748, 'learning_rate': 0.0008950000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.1787, 'learning_rate': 0.0008925, 'epoch': 0.03}\n",
      "{'loss': 0.1777, 'learning_rate': 0.0008900000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.1943, 'learning_rate': 0.0008874999999999999, 'epoch': 0.03}\n",
      "{'loss': 0.1875, 'learning_rate': 0.000885, 'epoch': 0.03}\n",
      "{'loss': 0.1592, 'learning_rate': 0.0008824999999999999, 'epoch': 0.03}\n",
      "{'loss': 0.1709, 'learning_rate': 0.00088, 'epoch': 0.03}\n",
      "{'loss': 0.1758, 'learning_rate': 0.0008774999999999999, 'epoch': 0.03}\n",
      "{'loss': 0.1719, 'learning_rate': 0.000875, 'epoch': 0.03}\n",
      "{'loss': 0.166, 'learning_rate': 0.0008725000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.1592, 'learning_rate': 0.00087, 'epoch': 0.03}\n",
      "{'loss': 0.248, 'learning_rate': 0.0008675000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.1846, 'learning_rate': 0.000865, 'epoch': 0.03}\n",
      "{'loss': 0.2031, 'learning_rate': 0.0008625000000000001, 'epoch': 0.04}\n",
      "{'loss': 0.1914, 'learning_rate': 0.00086, 'epoch': 0.04}\n",
      "{'loss': 0.1309, 'learning_rate': 0.0008575000000000001, 'epoch': 0.04}\n",
      "{'loss': 0.1631, 'learning_rate': 0.000855, 'epoch': 0.04}\n",
      "{'loss': 0.1465, 'learning_rate': 0.0008525000000000001, 'epoch': 0.04}\n",
      "{'loss': 0.1621, 'learning_rate': 0.00085, 'epoch': 0.04}\n",
      "{'loss': 0.2041, 'learning_rate': 0.0008475000000000001, 'epoch': 0.04}\n",
      "{'loss': 0.1523, 'learning_rate': 0.0008449999999999999, 'epoch': 0.04}\n",
      "{'loss': 0.1445, 'learning_rate': 0.0008425, 'epoch': 0.04}\n",
      "{'loss': 0.293, 'learning_rate': 0.00084, 'epoch': 0.04}\n",
      "{'loss': 0.1099, 'learning_rate': 0.0008375, 'epoch': 0.04}\n",
      "{'loss': 0.209, 'learning_rate': 0.000835, 'epoch': 0.04}\n",
      "{'loss': 0.1348, 'learning_rate': 0.0008325, 'epoch': 0.04}\n",
      "{'loss': 0.1533, 'learning_rate': 0.00083, 'epoch': 0.04}\n",
      "{'loss': 0.1445, 'learning_rate': 0.0008275, 'epoch': 0.04}\n",
      "{'loss': 0.1338, 'learning_rate': 0.000825, 'epoch': 0.04}\n",
      "{'loss': 0.1514, 'learning_rate': 0.0008225, 'epoch': 0.05}\n",
      "{'loss': 0.1318, 'learning_rate': 0.00082, 'epoch': 0.05}\n",
      "{'loss': 0.0991, 'learning_rate': 0.0008175, 'epoch': 0.05}\n",
      "{'loss': 0.1553, 'learning_rate': 0.000815, 'epoch': 0.05}\n",
      "{'loss': 0.1758, 'learning_rate': 0.0008125000000000001, 'epoch': 0.05}\n",
      "{'loss': 0.1543, 'learning_rate': 0.0008100000000000001, 'epoch': 0.05}\n",
      "{'loss': 0.1826, 'learning_rate': 0.0008075000000000001, 'epoch': 0.05}\n",
      "{'loss': 0.1582, 'learning_rate': 0.000805, 'epoch': 0.05}\n",
      "{'loss': 0.2256, 'learning_rate': 0.0008025, 'epoch': 0.05}\n",
      "{'loss': 0.2129, 'learning_rate': 0.0008, 'epoch': 0.05}\n",
      "{'loss': 0.1309, 'learning_rate': 0.0007975, 'epoch': 0.05}\n",
      "{'loss': 0.1138, 'learning_rate': 0.000795, 'epoch': 0.05}\n",
      "{'loss': 0.1982, 'learning_rate': 0.0007925, 'epoch': 0.05}\n",
      "{'loss': 0.1719, 'learning_rate': 0.00079, 'epoch': 0.05}\n",
      "{'loss': 0.1357, 'learning_rate': 0.0007875, 'epoch': 0.05}\n",
      "{'loss': 0.1406, 'learning_rate': 0.000785, 'epoch': 0.06}\n",
      "{'loss': 0.1504, 'learning_rate': 0.0007825, 'epoch': 0.06}\n",
      "{'loss': 0.1943, 'learning_rate': 0.0007800000000000001, 'epoch': 0.06}\n",
      "{'loss': 0.1523, 'learning_rate': 0.0007775, 'epoch': 0.06}\n",
      "{'loss': 0.1299, 'learning_rate': 0.0007750000000000001, 'epoch': 0.06}\n",
      "{'loss': 0.1387, 'learning_rate': 0.0007725, 'epoch': 0.06}\n",
      "{'loss': 0.21, 'learning_rate': 0.0007700000000000001, 'epoch': 0.06}\n",
      "{'loss': 0.2002, 'learning_rate': 0.0007675, 'epoch': 0.06}\n",
      "{'loss': 0.1416, 'learning_rate': 0.0007650000000000001, 'epoch': 0.06}\n",
      "{'loss': 0.1377, 'learning_rate': 0.0007624999999999999, 'epoch': 0.06}\n",
      "{'loss': 0.1367, 'learning_rate': 0.00076, 'epoch': 0.06}\n",
      "{'loss': 0.1108, 'learning_rate': 0.0007574999999999999, 'epoch': 0.06}\n",
      "{'loss': 0.1289, 'learning_rate': 0.000755, 'epoch': 0.06}\n",
      "{'loss': 0.1992, 'learning_rate': 0.0007524999999999999, 'epoch': 0.06}\n",
      "{'loss': 0.1235, 'learning_rate': 0.00075, 'epoch': 0.06}\n",
      "{'loss': 0.1582, 'learning_rate': 0.0007475000000000001, 'epoch': 0.06}\n",
      "{'loss': 0.1504, 'learning_rate': 0.000745, 'epoch': 0.07}\n",
      "{'loss': 0.1348, 'learning_rate': 0.0007425000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.2266, 'learning_rate': 0.00074, 'epoch': 0.07}\n",
      "{'loss': 0.1357, 'learning_rate': 0.0007375000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.1216, 'learning_rate': 0.000735, 'epoch': 0.07}\n",
      "{'loss': 0.1357, 'learning_rate': 0.0007325000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.1133, 'learning_rate': 0.00073, 'epoch': 0.07}\n",
      "{'loss': 0.126, 'learning_rate': 0.0007275000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.106, 'learning_rate': 0.000725, 'epoch': 0.07}\n",
      "{'loss': 0.2041, 'learning_rate': 0.0007225, 'epoch': 0.07}\n",
      "{'loss': 0.1182, 'learning_rate': 0.0007199999999999999, 'epoch': 0.07}\n",
      "{'loss': 0.2129, 'learning_rate': 0.0007175, 'epoch': 0.07}\n",
      "{'loss': 0.1289, 'learning_rate': 0.000715, 'epoch': 0.07}\n",
      "{'loss': 0.1406, 'learning_rate': 0.0007125, 'epoch': 0.07}\n",
      "{'loss': 0.1826, 'learning_rate': 0.00071, 'epoch': 0.07}\n",
      "{'loss': 0.1309, 'learning_rate': 0.0007075, 'epoch': 0.08}\n",
      "{'loss': 0.1816, 'learning_rate': 0.000705, 'epoch': 0.08}\n",
      "{'loss': 0.1367, 'learning_rate': 0.0007025, 'epoch': 0.08}\n",
      "{'loss': 0.1328, 'learning_rate': 0.0007, 'epoch': 0.08}\n",
      "{'loss': 0.1035, 'learning_rate': 0.0006975, 'epoch': 0.08}\n",
      "{'loss': 0.1118, 'learning_rate': 0.000695, 'epoch': 0.08}\n",
      "{'loss': 0.1201, 'learning_rate': 0.0006925, 'epoch': 0.08}\n",
      "{'loss': 0.1484, 'learning_rate': 0.00069, 'epoch': 0.08}\n",
      "{'loss': 0.1426, 'learning_rate': 0.0006875, 'epoch': 0.08}\n",
      "{'loss': 0.1221, 'learning_rate': 0.0006850000000000001, 'epoch': 0.08}\n",
      "{'loss': 0.1074, 'learning_rate': 0.0006825000000000001, 'epoch': 0.08}\n",
      "{'loss': 0.1206, 'learning_rate': 0.00068, 'epoch': 0.08}\n",
      "{'loss': 0.124, 'learning_rate': 0.0006775, 'epoch': 0.08}\n",
      "{'loss': 0.1128, 'learning_rate': 0.000675, 'epoch': 0.08}\n",
      "{'loss': 0.1299, 'learning_rate': 0.0006725, 'epoch': 0.08}\n",
      "{'loss': 0.105, 'learning_rate': 0.00067, 'epoch': 0.08}\n",
      "{'loss': 0.1128, 'learning_rate': 0.0006675, 'epoch': 0.09}\n",
      "{'loss': 0.1289, 'learning_rate': 0.000665, 'epoch': 0.09}\n",
      "{'loss': 0.1504, 'learning_rate': 0.0006625, 'epoch': 0.09}\n",
      "{'loss': 0.1221, 'learning_rate': 0.00066, 'epoch': 0.09}\n",
      "{'loss': 0.0996, 'learning_rate': 0.0006575, 'epoch': 0.09}\n",
      "{'loss': 0.2139, 'learning_rate': 0.0006550000000000001, 'epoch': 0.09}\n",
      "{'loss': 0.1235, 'learning_rate': 0.0006525, 'epoch': 0.09}\n",
      "{'loss': 0.168, 'learning_rate': 0.0006500000000000001, 'epoch': 0.09}\n",
      "{'loss': 0.1021, 'learning_rate': 0.0006475, 'epoch': 0.09}\n",
      "{'loss': 0.1289, 'learning_rate': 0.0006450000000000001, 'epoch': 0.09}\n",
      "{'loss': 0.1357, 'learning_rate': 0.0006425, 'epoch': 0.09}\n",
      "{'loss': 0.1289, 'learning_rate': 0.00064, 'epoch': 0.09}\n",
      "{'loss': 0.082, 'learning_rate': 0.0006374999999999999, 'epoch': 0.09}\n",
      "{'loss': 0.1011, 'learning_rate': 0.000635, 'epoch': 0.09}\n",
      "{'loss': 0.1221, 'learning_rate': 0.0006324999999999999, 'epoch': 0.09}\n",
      "{'loss': 0.1475, 'learning_rate': 0.00063, 'epoch': 0.09}\n",
      "{'loss': 0.123, 'learning_rate': 0.0006274999999999999, 'epoch': 0.1}\n",
      "{'loss': 0.1079, 'learning_rate': 0.000625, 'epoch': 0.1}\n",
      "{'loss': 0.1396, 'learning_rate': 0.0006225000000000001, 'epoch': 0.1}\n",
      "{'loss': 0.1289, 'learning_rate': 0.00062, 'epoch': 0.1}\n",
      "{'loss': 0.1436, 'learning_rate': 0.0006175000000000001, 'epoch': 0.1}\n",
      "{'loss': 0.2188, 'learning_rate': 0.000615, 'epoch': 0.1}\n",
      "{'loss': 0.1128, 'learning_rate': 0.0006125000000000001, 'epoch': 0.1}\n",
      "{'loss': 0.1553, 'learning_rate': 0.00061, 'epoch': 0.1}\n",
      "{'loss': 0.1387, 'learning_rate': 0.0006075000000000001, 'epoch': 0.1}\n",
      "{'loss': 0.1475, 'learning_rate': 0.000605, 'epoch': 0.1}\n",
      "{'loss': 0.1099, 'learning_rate': 0.0006025000000000001, 'epoch': 0.1}\n",
      "{'loss': 0.126, 'learning_rate': 0.0006, 'epoch': 0.1}\n",
      "{'loss': 0.1494, 'learning_rate': 0.0005975, 'epoch': 0.1}\n",
      "{'loss': 0.1904, 'learning_rate': 0.0005949999999999999, 'epoch': 0.1}\n",
      "{'loss': 0.1738, 'learning_rate': 0.0005925, 'epoch': 0.1}\n",
      "{'loss': 0.1147, 'learning_rate': 0.00059, 'epoch': 0.11}\n",
      "{'loss': 0.1553, 'learning_rate': 0.0005875, 'epoch': 0.11}\n",
      "{'loss': 0.1406, 'learning_rate': 0.000585, 'epoch': 0.11}\n",
      "{'loss': 0.127, 'learning_rate': 0.0005825, 'epoch': 0.11}\n",
      "{'loss': 0.1621, 'learning_rate': 0.00058, 'epoch': 0.11}\n",
      "{'loss': 0.1191, 'learning_rate': 0.0005775, 'epoch': 0.11}\n",
      "{'loss': 0.1035, 'learning_rate': 0.000575, 'epoch': 0.11}\n",
      "{'loss': 0.106, 'learning_rate': 0.0005725, 'epoch': 0.11}\n",
      "{'loss': 0.1143, 'learning_rate': 0.00057, 'epoch': 0.11}\n",
      "{'loss': 0.1768, 'learning_rate': 0.0005675, 'epoch': 0.11}\n",
      "{'loss': 0.166, 'learning_rate': 0.000565, 'epoch': 0.11}\n",
      "{'loss': 0.1582, 'learning_rate': 0.0005625000000000001, 'epoch': 0.11}\n",
      "{'loss': 0.1543, 'learning_rate': 0.0005600000000000001, 'epoch': 0.11}\n",
      "{'loss': 0.1396, 'learning_rate': 0.0005575, 'epoch': 0.11}\n",
      "{'loss': 0.0986, 'learning_rate': 0.000555, 'epoch': 0.11}\n",
      "{'loss': 0.1904, 'learning_rate': 0.0005525, 'epoch': 0.11}\n",
      "{'loss': 0.1406, 'learning_rate': 0.00055, 'epoch': 0.12}\n",
      "{'loss': 0.1328, 'learning_rate': 0.0005475, 'epoch': 0.12}\n",
      "{'loss': 0.1611, 'learning_rate': 0.000545, 'epoch': 0.12}\n",
      "{'loss': 0.1123, 'learning_rate': 0.0005425, 'epoch': 0.12}\n",
      "{'loss': 0.1328, 'learning_rate': 0.00054, 'epoch': 0.12}\n",
      "{'loss': 0.1572, 'learning_rate': 0.0005375, 'epoch': 0.12}\n",
      "{'loss': 0.1089, 'learning_rate': 0.000535, 'epoch': 0.12}\n",
      "{'loss': 0.124, 'learning_rate': 0.0005325, 'epoch': 0.12}\n",
      "{'loss': 0.1875, 'learning_rate': 0.0005300000000000001, 'epoch': 0.12}\n",
      "{'loss': 0.1504, 'learning_rate': 0.0005275, 'epoch': 0.12}\n",
      "{'loss': 0.1108, 'learning_rate': 0.0005250000000000001, 'epoch': 0.12}\n",
      "{'loss': 0.1602, 'learning_rate': 0.0005225, 'epoch': 0.12}\n",
      "{'loss': 0.1631, 'learning_rate': 0.0005200000000000001, 'epoch': 0.12}\n",
      "{'loss': 0.1494, 'learning_rate': 0.0005175, 'epoch': 0.12}\n",
      "{'loss': 0.1455, 'learning_rate': 0.000515, 'epoch': 0.12}\n",
      "{'loss': 0.103, 'learning_rate': 0.0005124999999999999, 'epoch': 0.13}\n",
      "{'loss': 0.1104, 'learning_rate': 0.00051, 'epoch': 0.13}\n",
      "{'loss': 0.1387, 'learning_rate': 0.0005074999999999999, 'epoch': 0.13}\n",
      "{'loss': 0.0967, 'learning_rate': 0.000505, 'epoch': 0.13}\n",
      "{'loss': 0.1079, 'learning_rate': 0.0005024999999999999, 'epoch': 0.13}\n",
      "{'loss': 0.1216, 'learning_rate': 0.0005, 'epoch': 0.13}\n",
      "{'loss': 0.1348, 'learning_rate': 0.0004975, 'epoch': 0.13}\n",
      "{'loss': 0.1152, 'learning_rate': 0.000495, 'epoch': 0.13}\n",
      "{'loss': 0.1533, 'learning_rate': 0.0004925, 'epoch': 0.13}\n",
      "{'loss': 0.0967, 'learning_rate': 0.00049, 'epoch': 0.13}\n",
      "{'loss': 0.1201, 'learning_rate': 0.0004875, 'epoch': 0.13}\n",
      "{'loss': 0.165, 'learning_rate': 0.00048499999999999997, 'epoch': 0.13}\n",
      "{'loss': 0.1094, 'learning_rate': 0.0004825, 'epoch': 0.13}\n",
      "{'loss': 0.1387, 'learning_rate': 0.00048, 'epoch': 0.13}\n",
      "{'loss': 0.0913, 'learning_rate': 0.0004775, 'epoch': 0.13}\n",
      "{'loss': 0.127, 'learning_rate': 0.000475, 'epoch': 0.13}\n",
      "{'loss': 0.1162, 'learning_rate': 0.0004725, 'epoch': 0.14}\n",
      "{'loss': 0.167, 'learning_rate': 0.00047, 'epoch': 0.14}\n",
      "{'loss': 0.1387, 'learning_rate': 0.00046750000000000003, 'epoch': 0.14}\n",
      "{'loss': 0.1484, 'learning_rate': 0.000465, 'epoch': 0.14}\n",
      "{'loss': 0.1006, 'learning_rate': 0.0004625, 'epoch': 0.14}\n",
      "{'loss': 0.1328, 'learning_rate': 0.00046, 'epoch': 0.14}\n",
      "{'loss': 0.1904, 'learning_rate': 0.0004575, 'epoch': 0.14}\n",
      "{'loss': 0.1377, 'learning_rate': 0.000455, 'epoch': 0.14}\n",
      "{'loss': 0.1377, 'learning_rate': 0.00045250000000000005, 'epoch': 0.14}\n",
      "{'loss': 0.106, 'learning_rate': 0.00045000000000000004, 'epoch': 0.14}\n",
      "{'loss': 0.1201, 'learning_rate': 0.00044750000000000004, 'epoch': 0.14}\n",
      "{'loss': 0.1006, 'learning_rate': 0.00044500000000000003, 'epoch': 0.14}\n",
      "{'loss': 0.105, 'learning_rate': 0.0004425, 'epoch': 0.14}\n",
      "{'loss': 0.1289, 'learning_rate': 0.00044, 'epoch': 0.14}\n",
      "{'loss': 0.1348, 'learning_rate': 0.0004375, 'epoch': 0.14}\n",
      "{'loss': 0.1611, 'learning_rate': 0.000435, 'epoch': 0.15}\n",
      "{'loss': 0.0977, 'learning_rate': 0.0004325, 'epoch': 0.15}\n",
      "{'loss': 0.1128, 'learning_rate': 0.00043, 'epoch': 0.15}\n",
      "{'loss': 0.1104, 'learning_rate': 0.0004275, 'epoch': 0.15}\n",
      "{'loss': 0.1191, 'learning_rate': 0.000425, 'epoch': 0.15}\n",
      "{'loss': 0.1279, 'learning_rate': 0.00042249999999999997, 'epoch': 0.15}\n",
      "{'loss': 0.1309, 'learning_rate': 0.00042, 'epoch': 0.15}\n",
      "{'loss': 0.0938, 'learning_rate': 0.0004175, 'epoch': 0.15}\n",
      "{'loss': 0.126, 'learning_rate': 0.000415, 'epoch': 0.15}\n",
      "{'loss': 0.1113, 'learning_rate': 0.0004125, 'epoch': 0.15}\n",
      "{'loss': 0.1494, 'learning_rate': 0.00041, 'epoch': 0.15}\n",
      "{'loss': 0.1108, 'learning_rate': 0.0004075, 'epoch': 0.15}\n",
      "{'loss': 0.1484, 'learning_rate': 0.00040500000000000003, 'epoch': 0.15}\n",
      "{'loss': 0.1187, 'learning_rate': 0.0004025, 'epoch': 0.15}\n",
      "{'loss': 0.1406, 'learning_rate': 0.0004, 'epoch': 0.15}\n",
      "{'loss': 0.1099, 'learning_rate': 0.0003975, 'epoch': 0.15}\n",
      "{'loss': 0.124, 'learning_rate': 0.000395, 'epoch': 0.16}\n",
      "{'loss': 0.1436, 'learning_rate': 0.0003925, 'epoch': 0.16}\n",
      "{'loss': 0.124, 'learning_rate': 0.00039000000000000005, 'epoch': 0.16}\n",
      "{'loss': 0.1484, 'learning_rate': 0.00038750000000000004, 'epoch': 0.16}\n",
      "{'loss': 0.1094, 'learning_rate': 0.00038500000000000003, 'epoch': 0.16}\n",
      "{'loss': 0.0669, 'learning_rate': 0.00038250000000000003, 'epoch': 0.16}\n",
      "{'loss': 0.1299, 'learning_rate': 0.00038, 'epoch': 0.16}\n",
      "{'loss': 0.1089, 'learning_rate': 0.0003775, 'epoch': 0.16}\n",
      "{'loss': 0.1094, 'learning_rate': 0.000375, 'epoch': 0.16}\n",
      "{'loss': 0.1396, 'learning_rate': 0.0003725, 'epoch': 0.16}\n",
      "{'loss': 0.1738, 'learning_rate': 0.00037, 'epoch': 0.16}\n",
      "{'loss': 0.1201, 'learning_rate': 0.0003675, 'epoch': 0.16}\n",
      "{'loss': 0.1367, 'learning_rate': 0.000365, 'epoch': 0.16}\n",
      "{'loss': 0.1187, 'learning_rate': 0.0003625, 'epoch': 0.16}\n",
      "{'loss': 0.1426, 'learning_rate': 0.00035999999999999997, 'epoch': 0.16}\n",
      "{'loss': 0.1289, 'learning_rate': 0.0003575, 'epoch': 0.16}\n",
      "{'loss': 0.1738, 'learning_rate': 0.000355, 'epoch': 0.17}\n",
      "{'loss': 0.1709, 'learning_rate': 0.0003525, 'epoch': 0.17}\n",
      "{'loss': 0.1162, 'learning_rate': 0.00035, 'epoch': 0.17}\n",
      "{'loss': 0.1328, 'learning_rate': 0.0003475, 'epoch': 0.17}\n",
      "{'loss': 0.1162, 'learning_rate': 0.000345, 'epoch': 0.17}\n",
      "{'loss': 0.1299, 'learning_rate': 0.00034250000000000003, 'epoch': 0.17}\n",
      "{'loss': 0.1167, 'learning_rate': 0.00034, 'epoch': 0.17}\n",
      "{'loss': 0.1357, 'learning_rate': 0.0003375, 'epoch': 0.17}\n",
      "{'loss': 0.127, 'learning_rate': 0.000335, 'epoch': 0.17}\n",
      "{'loss': 0.1406, 'learning_rate': 0.0003325, 'epoch': 0.17}\n",
      "{'loss': 0.1113, 'learning_rate': 0.00033, 'epoch': 0.17}\n",
      "{'loss': 0.1216, 'learning_rate': 0.00032750000000000005, 'epoch': 0.17}\n",
      "{'loss': 0.127, 'learning_rate': 0.00032500000000000004, 'epoch': 0.17}\n",
      "{'loss': 0.106, 'learning_rate': 0.00032250000000000003, 'epoch': 0.17}\n",
      "{'loss': 0.1162, 'learning_rate': 0.00032, 'epoch': 0.17}\n",
      "{'loss': 0.1367, 'learning_rate': 0.0003175, 'epoch': 0.18}\n",
      "{'loss': 0.1128, 'learning_rate': 0.000315, 'epoch': 0.18}\n",
      "{'loss': 0.126, 'learning_rate': 0.0003125, 'epoch': 0.18}\n",
      "{'loss': 0.1147, 'learning_rate': 0.00031, 'epoch': 0.18}\n",
      "{'loss': 0.1245, 'learning_rate': 0.0003075, 'epoch': 0.18}\n",
      "{'loss': 0.1108, 'learning_rate': 0.000305, 'epoch': 0.18}\n",
      "{'loss': 0.1289, 'learning_rate': 0.0003025, 'epoch': 0.18}\n",
      "{'loss': 0.124, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
      "{'loss': 0.1289, 'learning_rate': 0.00029749999999999997, 'epoch': 0.18}\n",
      "{'loss': 0.1455, 'learning_rate': 0.000295, 'epoch': 0.18}\n",
      "{'loss': 0.0952, 'learning_rate': 0.0002925, 'epoch': 0.18}\n",
      "{'loss': 0.1318, 'learning_rate': 0.00029, 'epoch': 0.18}\n",
      "{'loss': 0.1348, 'learning_rate': 0.0002875, 'epoch': 0.18}\n",
      "{'loss': 0.1089, 'learning_rate': 0.000285, 'epoch': 0.18}\n",
      "{'loss': 0.104, 'learning_rate': 0.0002825, 'epoch': 0.18}\n",
      "{'loss': 0.1133, 'learning_rate': 0.00028000000000000003, 'epoch': 0.18}\n",
      "{'loss': 0.1484, 'learning_rate': 0.0002775, 'epoch': 0.19}\n",
      "{'loss': 0.1152, 'learning_rate': 0.000275, 'epoch': 0.19}\n",
      "{'loss': 0.1338, 'learning_rate': 0.0002725, 'epoch': 0.19}\n",
      "{'loss': 0.1309, 'learning_rate': 0.00027, 'epoch': 0.19}\n",
      "{'loss': 0.1104, 'learning_rate': 0.0002675, 'epoch': 0.19}\n",
      "{'loss': 0.1504, 'learning_rate': 0.00026500000000000004, 'epoch': 0.19}\n",
      "{'loss': 0.127, 'learning_rate': 0.00026250000000000004, 'epoch': 0.19}\n",
      "{'loss': 0.1055, 'learning_rate': 0.00026000000000000003, 'epoch': 0.19}\n",
      "{'loss': 0.126, 'learning_rate': 0.0002575, 'epoch': 0.19}\n",
      "{'loss': 0.127, 'learning_rate': 0.000255, 'epoch': 0.19}\n",
      "{'loss': 0.1338, 'learning_rate': 0.0002525, 'epoch': 0.19}\n",
      "{'loss': 0.1455, 'learning_rate': 0.00025, 'epoch': 0.19}\n",
      "{'loss': 0.1357, 'learning_rate': 0.0002475, 'epoch': 0.19}\n",
      "{'loss': 0.0869, 'learning_rate': 0.000245, 'epoch': 0.19}\n",
      "{'loss': 0.1191, 'learning_rate': 0.00024249999999999999, 'epoch': 0.19}\n",
      "{'loss': 0.1216, 'learning_rate': 0.00024, 'epoch': 0.2}\n",
      "{'loss': 0.1084, 'learning_rate': 0.0002375, 'epoch': 0.2}\n",
      "{'loss': 0.1245, 'learning_rate': 0.000235, 'epoch': 0.2}\n",
      "{'loss': 0.1099, 'learning_rate': 0.0002325, 'epoch': 0.2}\n",
      "{'loss': 0.1611, 'learning_rate': 0.00023, 'epoch': 0.2}\n",
      "{'loss': 0.1104, 'learning_rate': 0.0002275, 'epoch': 0.2}\n",
      "{'loss': 0.0923, 'learning_rate': 0.00022500000000000002, 'epoch': 0.2}\n",
      "{'loss': 0.1475, 'learning_rate': 0.00022250000000000001, 'epoch': 0.2}\n",
      "{'loss': 0.1494, 'learning_rate': 0.00022, 'epoch': 0.2}\n",
      "{'loss': 0.1084, 'learning_rate': 0.0002175, 'epoch': 0.2}\n",
      "{'loss': 0.1147, 'learning_rate': 0.000215, 'epoch': 0.2}\n",
      "{'loss': 0.1172, 'learning_rate': 0.0002125, 'epoch': 0.2}\n",
      "{'loss': 0.1104, 'learning_rate': 0.00021, 'epoch': 0.2}\n",
      "{'loss': 0.1162, 'learning_rate': 0.0002075, 'epoch': 0.2}\n",
      "{'loss': 0.1299, 'learning_rate': 0.000205, 'epoch': 0.2}\n",
      "{'loss': 0.1357, 'learning_rate': 0.00020250000000000002, 'epoch': 0.2}\n",
      "{'loss': 0.1133, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.1289, 'learning_rate': 0.0001975, 'epoch': 0.21}\n",
      "{'loss': 0.105, 'learning_rate': 0.00019500000000000002, 'epoch': 0.21}\n",
      "{'loss': 0.104, 'learning_rate': 0.00019250000000000002, 'epoch': 0.21}\n",
      "{'loss': 0.1328, 'learning_rate': 0.00019, 'epoch': 0.21}\n",
      "{'loss': 0.1074, 'learning_rate': 0.0001875, 'epoch': 0.21}\n",
      "{'loss': 0.106, 'learning_rate': 0.000185, 'epoch': 0.21}\n",
      "{'loss': 0.1108, 'learning_rate': 0.0001825, 'epoch': 0.21}\n",
      "{'loss': 0.1211, 'learning_rate': 0.00017999999999999998, 'epoch': 0.21}\n",
      "{'loss': 0.1738, 'learning_rate': 0.0001775, 'epoch': 0.21}\n",
      "{'loss': 0.1455, 'learning_rate': 0.000175, 'epoch': 0.21}\n",
      "{'loss': 0.1309, 'learning_rate': 0.0001725, 'epoch': 0.21}\n",
      "{'loss': 0.0923, 'learning_rate': 0.00017, 'epoch': 0.21}\n",
      "{'loss': 0.0845, 'learning_rate': 0.0001675, 'epoch': 0.21}\n",
      "{'loss': 0.1016, 'learning_rate': 0.000165, 'epoch': 0.21}\n",
      "{'loss': 0.1182, 'learning_rate': 0.00016250000000000002, 'epoch': 0.22}\n",
      "{'loss': 0.1035, 'learning_rate': 0.00016, 'epoch': 0.22}\n",
      "{'loss': 0.1406, 'learning_rate': 0.0001575, 'epoch': 0.22}\n",
      "{'loss': 0.1279, 'learning_rate': 0.000155, 'epoch': 0.22}\n",
      "{'loss': 0.125, 'learning_rate': 0.0001525, 'epoch': 0.22}\n",
      "{'loss': 0.0947, 'learning_rate': 0.00015, 'epoch': 0.22}\n",
      "{'loss': 0.1445, 'learning_rate': 0.0001475, 'epoch': 0.22}\n",
      "{'loss': 0.1108, 'learning_rate': 0.000145, 'epoch': 0.22}\n",
      "{'loss': 0.1187, 'learning_rate': 0.0001425, 'epoch': 0.22}\n",
      "{'loss': 0.1396, 'learning_rate': 0.00014000000000000001, 'epoch': 0.22}\n",
      "{'loss': 0.1025, 'learning_rate': 0.0001375, 'epoch': 0.22}\n",
      "{'loss': 0.1416, 'learning_rate': 0.000135, 'epoch': 0.22}\n",
      "{'loss': 0.1201, 'learning_rate': 0.00013250000000000002, 'epoch': 0.22}\n",
      "{'loss': 0.1465, 'learning_rate': 0.00013000000000000002, 'epoch': 0.22}\n",
      "{'loss': 0.0952, 'learning_rate': 0.0001275, 'epoch': 0.22}\n",
      "{'loss': 0.1299, 'learning_rate': 0.000125, 'epoch': 0.22}\n",
      "{'loss': 0.1187, 'learning_rate': 0.0001225, 'epoch': 0.23}\n",
      "{'loss': 0.1133, 'learning_rate': 0.00012, 'epoch': 0.23}\n",
      "{'loss': 0.1875, 'learning_rate': 0.0001175, 'epoch': 0.23}\n",
      "{'loss': 0.1553, 'learning_rate': 0.000115, 'epoch': 0.23}\n",
      "{'loss': 0.0859, 'learning_rate': 0.00011250000000000001, 'epoch': 0.23}\n",
      "{'loss': 0.1289, 'learning_rate': 0.00011, 'epoch': 0.23}\n",
      "{'loss': 0.1094, 'learning_rate': 0.0001075, 'epoch': 0.23}\n",
      "{'loss': 0.1191, 'learning_rate': 0.000105, 'epoch': 0.23}\n",
      "{'loss': 0.103, 'learning_rate': 0.0001025, 'epoch': 0.23}\n",
      "{'loss': 0.1001, 'learning_rate': 0.0001, 'epoch': 0.23}\n",
      "{'loss': 0.1108, 'learning_rate': 9.750000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1201, 'learning_rate': 9.5e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1143, 'learning_rate': 9.25e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1455, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0981, 'learning_rate': 8.75e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1289, 'learning_rate': 8.5e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1182, 'learning_rate': 8.25e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1006, 'learning_rate': 8e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0923, 'learning_rate': 7.75e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1641, 'learning_rate': 7.5e-05, 'epoch': 0.24}\n",
      "{'loss': 0.106, 'learning_rate': 7.25e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1201, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1553, 'learning_rate': 6.75e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1348, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1006, 'learning_rate': 6.25e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1025, 'learning_rate': 6e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1245, 'learning_rate': 5.75e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1406, 'learning_rate': 5.5e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1108, 'learning_rate': 5.25e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0996, 'learning_rate': 5e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1387, 'learning_rate': 4.75e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0747, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1123, 'learning_rate': 4.25e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0967, 'learning_rate': 4e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1143, 'learning_rate': 3.75e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1079, 'learning_rate': 3.5000000000000004e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1455, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1113, 'learning_rate': 3e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0854, 'learning_rate': 2.75e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1094, 'learning_rate': 2.5e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0942, 'learning_rate': 2.2499999999999998e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0972, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.125, 'learning_rate': 1.7500000000000002e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1299, 'learning_rate': 1.5e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1074, 'learning_rate': 1.25e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1592, 'learning_rate': 1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1299, 'learning_rate': 7.5e-06, 'epoch': 0.25}\n",
      "{'loss': 0.1016, 'learning_rate': 5e-06, 'epoch': 0.26}\n",
      "{'loss': 0.1309, 'learning_rate': 2.5e-06, 'epoch': 0.26}\n",
      "{'loss': 0.1182, 'learning_rate': 0.0, 'epoch': 0.26}\n",
      "{'train_runtime': 88.0268, 'train_samples_per_second': 36.353, 'train_steps_per_second': 4.544, 'train_loss': 0.148619384765625, 'epoch': 0.26}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.148619384765625, metrics={'train_runtime': 88.0268, 'train_samples_per_second': 36.353, 'train_steps_per_second': 4.544, 'train_loss': 0.148619384765625, 'epoch': 0.26})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = peft_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1# wants to upgrade his system and asks #Person2# to help him with upgrading his hardware.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person2# wants to upgrade the system and #Person1# suggests adding a painting program to #Person2#'s software. #Person2# suggests adding a CD-ROM drive and a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      " #Person2# wants to upgrade their system and their hardware. #Person1# suggests adding a painting program to their software. #Person2# also recommends a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.to(device).generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\"]\n",
    "config = LoraConfig(r=32, lora_alpha=32, target_modules=target_modules, lora_dropout=0.01, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "# rank = 32\n",
    "# model = transformers.GPTJForCausalLM.from_pretrained(\n",
    "#    \"kakaobrain/kogpt\",\n",
    "#     revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "#      pad_token_id=tokenizer.eos_token_id,\n",
    "#     use_cache=False,\n",
    "#      device_map={\"\": rank},\n",
    "#      torch_dtype=torch.float16,\n",
    "#      load_in_8bit=True,)\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 2419232\n",
      "all model parameters: 70933280\n",
      "percentage of trainable model parameters: 3.41%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(lora_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1,\n",
    "    max_steps=100   \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb. init(mode=\"disabled\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5dbd9e37ff40ef9e16946fb928a2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a741436c053140eeab61a4257c92c3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3094, 'learning_rate': 0.00099, 'epoch': 0.0}\n",
      "{'loss': 0.6848, 'learning_rate': 0.00098, 'epoch': 0.0}\n",
      "{'loss': 0.4427, 'learning_rate': 0.0009699999999999999, 'epoch': 0.0}\n",
      "{'loss': 0.377, 'learning_rate': 0.00096, 'epoch': 0.0}\n",
      "{'loss': 0.2299, 'learning_rate': 0.00095, 'epoch': 0.0}\n",
      "{'loss': 0.3646, 'learning_rate': 0.00094, 'epoch': 0.0}\n",
      "{'loss': 0.2827, 'learning_rate': 0.00093, 'epoch': 0.0}\n",
      "{'loss': 0.142, 'learning_rate': 0.00092, 'epoch': 0.0}\n",
      "{'loss': 0.2537, 'learning_rate': 0.00091, 'epoch': 0.0}\n",
      "{'loss': 0.2379, 'learning_rate': 0.0009000000000000001, 'epoch': 0.0}\n",
      "{'loss': 0.179, 'learning_rate': 0.0008900000000000001, 'epoch': 0.0}\n",
      "{'loss': 0.138, 'learning_rate': 0.00088, 'epoch': 0.0}\n",
      "{'loss': 0.3155, 'learning_rate': 0.00087, 'epoch': 0.0}\n",
      "{'loss': 0.2185, 'learning_rate': 0.00086, 'epoch': 0.0}\n",
      "{'loss': 0.2874, 'learning_rate': 0.00085, 'epoch': 0.0}\n",
      "{'loss': 0.2378, 'learning_rate': 0.00084, 'epoch': 0.01}\n",
      "{'loss': 0.2416, 'learning_rate': 0.00083, 'epoch': 0.01}\n",
      "{'loss': 0.1132, 'learning_rate': 0.00082, 'epoch': 0.01}\n",
      "{'loss': 0.1436, 'learning_rate': 0.0008100000000000001, 'epoch': 0.01}\n",
      "{'loss': 0.2176, 'learning_rate': 0.0008, 'epoch': 0.01}\n",
      "{'loss': 0.1048, 'learning_rate': 0.00079, 'epoch': 0.01}\n",
      "{'loss': 0.1398, 'learning_rate': 0.0007800000000000001, 'epoch': 0.01}\n",
      "{'loss': 0.1063, 'learning_rate': 0.0007700000000000001, 'epoch': 0.01}\n",
      "{'loss': 0.1307, 'learning_rate': 0.00076, 'epoch': 0.01}\n",
      "{'loss': 0.1452, 'learning_rate': 0.00075, 'epoch': 0.01}\n",
      "{'loss': 0.0996, 'learning_rate': 0.00074, 'epoch': 0.01}\n",
      "{'loss': 0.134, 'learning_rate': 0.00073, 'epoch': 0.01}\n",
      "{'loss': 0.1525, 'learning_rate': 0.0007199999999999999, 'epoch': 0.01}\n",
      "{'loss': 0.1487, 'learning_rate': 0.00071, 'epoch': 0.01}\n",
      "{'loss': 0.0896, 'learning_rate': 0.0007, 'epoch': 0.01}\n",
      "{'loss': 0.1054, 'learning_rate': 0.00069, 'epoch': 0.01}\n",
      "{'loss': 0.0907, 'learning_rate': 0.00068, 'epoch': 0.01}\n",
      "{'loss': 0.1261, 'learning_rate': 0.00067, 'epoch': 0.01}\n",
      "{'loss': 0.1294, 'learning_rate': 0.00066, 'epoch': 0.01}\n",
      "{'loss': 0.1121, 'learning_rate': 0.0006500000000000001, 'epoch': 0.01}\n",
      "{'loss': 0.1306, 'learning_rate': 0.00064, 'epoch': 0.01}\n",
      "{'loss': 0.1132, 'learning_rate': 0.00063, 'epoch': 0.01}\n",
      "{'loss': 0.1282, 'learning_rate': 0.00062, 'epoch': 0.01}\n",
      "{'loss': 0.0988, 'learning_rate': 0.00061, 'epoch': 0.01}\n",
      "{'loss': 0.1261, 'learning_rate': 0.0006, 'epoch': 0.01}\n",
      "{'loss': 0.0992, 'learning_rate': 0.00059, 'epoch': 0.01}\n",
      "{'loss': 0.0896, 'learning_rate': 0.00058, 'epoch': 0.01}\n",
      "{'loss': 0.0979, 'learning_rate': 0.00057, 'epoch': 0.01}\n",
      "{'loss': 0.1306, 'learning_rate': 0.0005600000000000001, 'epoch': 0.01}\n",
      "{'loss': 0.127, 'learning_rate': 0.00055, 'epoch': 0.01}\n",
      "{'loss': 0.1932, 'learning_rate': 0.00054, 'epoch': 0.01}\n",
      "{'loss': 0.1239, 'learning_rate': 0.0005300000000000001, 'epoch': 0.02}\n",
      "{'loss': 0.0979, 'learning_rate': 0.0005200000000000001, 'epoch': 0.02}\n",
      "{'loss': 0.1463, 'learning_rate': 0.00051, 'epoch': 0.02}\n",
      "{'loss': 0.1044, 'learning_rate': 0.0005, 'epoch': 0.02}\n",
      "{'loss': 0.1335, 'learning_rate': 0.00049, 'epoch': 0.02}\n",
      "{'loss': 0.1093, 'learning_rate': 0.00048, 'epoch': 0.02}\n",
      "{'loss': 0.1904, 'learning_rate': 0.00047, 'epoch': 0.02}\n",
      "{'loss': 0.0987, 'learning_rate': 0.00046, 'epoch': 0.02}\n",
      "{'loss': 0.0856, 'learning_rate': 0.00045000000000000004, 'epoch': 0.02}\n",
      "{'loss': 0.2503, 'learning_rate': 0.00044, 'epoch': 0.02}\n",
      "{'loss': 0.1187, 'learning_rate': 0.00043, 'epoch': 0.02}\n",
      "{'loss': 0.1382, 'learning_rate': 0.00042, 'epoch': 0.02}\n",
      "{'loss': 0.1622, 'learning_rate': 0.00041, 'epoch': 0.02}\n",
      "{'loss': 0.1259, 'learning_rate': 0.0004, 'epoch': 0.02}\n",
      "{'loss': 0.1081, 'learning_rate': 0.00039000000000000005, 'epoch': 0.02}\n",
      "{'loss': 0.1321, 'learning_rate': 0.00038, 'epoch': 0.02}\n",
      "{'loss': 0.1062, 'learning_rate': 0.00037, 'epoch': 0.02}\n",
      "{'loss': 0.1258, 'learning_rate': 0.00035999999999999997, 'epoch': 0.02}\n",
      "{'loss': 0.1596, 'learning_rate': 0.00035, 'epoch': 0.02}\n",
      "{'loss': 0.0904, 'learning_rate': 0.00034, 'epoch': 0.02}\n",
      "{'loss': 0.0996, 'learning_rate': 0.00033, 'epoch': 0.02}\n",
      "{'loss': 0.1673, 'learning_rate': 0.00032, 'epoch': 0.02}\n",
      "{'loss': 0.1133, 'learning_rate': 0.00031, 'epoch': 0.02}\n",
      "{'loss': 0.1521, 'learning_rate': 0.0003, 'epoch': 0.02}\n",
      "{'loss': 0.1224, 'learning_rate': 0.00029, 'epoch': 0.02}\n",
      "{'loss': 0.1188, 'learning_rate': 0.00028000000000000003, 'epoch': 0.02}\n",
      "{'loss': 0.1276, 'learning_rate': 0.00027, 'epoch': 0.02}\n",
      "{'loss': 0.1094, 'learning_rate': 0.00026000000000000003, 'epoch': 0.02}\n",
      "{'loss': 0.1183, 'learning_rate': 0.00025, 'epoch': 0.02}\n",
      "{'loss': 0.087, 'learning_rate': 0.00024, 'epoch': 0.02}\n",
      "{'loss': 0.1372, 'learning_rate': 0.00023, 'epoch': 0.02}\n",
      "{'loss': 0.1258, 'learning_rate': 0.00022, 'epoch': 0.03}\n",
      "{'loss': 0.1086, 'learning_rate': 0.00021, 'epoch': 0.03}\n",
      "{'loss': 0.11, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 0.1725, 'learning_rate': 0.00019, 'epoch': 0.03}\n",
      "{'loss': 0.0801, 'learning_rate': 0.00017999999999999998, 'epoch': 0.03}\n",
      "{'loss': 0.0843, 'learning_rate': 0.00017, 'epoch': 0.03}\n",
      "{'loss': 0.1139, 'learning_rate': 0.00016, 'epoch': 0.03}\n",
      "{'loss': 0.1111, 'learning_rate': 0.00015, 'epoch': 0.03}\n",
      "{'loss': 0.1305, 'learning_rate': 0.00014000000000000001, 'epoch': 0.03}\n",
      "{'loss': 0.0842, 'learning_rate': 0.00013000000000000002, 'epoch': 0.03}\n",
      "{'loss': 0.0939, 'learning_rate': 0.00012, 'epoch': 0.03}\n",
      "{'loss': 0.1085, 'learning_rate': 0.00011, 'epoch': 0.03}\n",
      "{'loss': 0.0947, 'learning_rate': 0.0001, 'epoch': 0.03}\n",
      "{'loss': 0.1051, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1359, 'learning_rate': 8e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1957, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1289, 'learning_rate': 6e-05, 'epoch': 0.03}\n",
      "{'loss': 0.083, 'learning_rate': 5e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1071, 'learning_rate': 4e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1017, 'learning_rate': 3e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1168, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0821, 'learning_rate': 1e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0682, 'learning_rate': 0.0, 'epoch': 0.03}\n",
      "{'train_runtime': 23.3152, 'train_samples_per_second': 17.156, 'train_steps_per_second': 4.289, 'train_loss': 0.23069281838834285, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.23069281838834285, metrics={'train_runtime': 23.3152, 'train_samples_per_second': 17.156, 'train_steps_per_second': 4.289, 'train_loss': 0.23069281838834285, 'epoch': 0.03})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft/tokenizer_config.json',\n",
       " './peft/special_tokens_map.json',\n",
       " './peft/vocab.json',\n",
       " './peft/merges.txt',\n",
       " './peft/added_tokens.json',\n",
       " './peft/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = peft_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' one day, he was to to the man, to to to the man, and, to to to to to to the man, and to to to to.,'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ''' one day'''\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    lora_model.generate(\n",
    "        # inputs[\"input_ids\"], \n",
    "        input_ids=inputs.input_ids.to('cuda'),\n",
    "        max_new_tokens=200,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "original_model_outputs = peft_trainer.model.generate(input_ids=input_ids, pad_token_id=tokenizer.eos_token_id, generation_config=GenerationConfig(max_new_tokens=2009, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSummarize the following conversation.\\n\\n#Person1#: Have you considered upgrading your system?\\n#Person2#: Yes, but I'm not sure what exactly I would need.\\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\\n#Person2#: That would be a definite bonus.\\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\\n#Person2#: How can we do that?\\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\\n#Person2#: No.\\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\\n#Person2#: That sounds great. Thanks.\\n\\nSummary: \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1000   \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
