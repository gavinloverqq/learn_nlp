{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者：孙文奇\n",
    "链接：https://zhuanlan.zhihu.com/p/361431647\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "在PyTorch中，针对词向量有一个专门的层nn.Embedding，用来实现词与词向量的映射。nn.Embedding具有一个权重（.weight），形状是(num_words, embedding_dim)。例如一共有10个词，每个词用2维向量表征，对应的权重就是一个10×2的矩阵。Embedding的输入形状N×W，N是batch size，W是序列的长度，输出的形状是N×W×embedding_dim。输入必须是LongTensor，FloatTensor需通过tensor.long()方法转成LongTensor。Embedding的权重是可以训练的，既可以采用随机初始化，也可以采用预训练好的词向量初始化。# coding:utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embedding = nn.Embedding(10, 2)  # 10个词，每个词用2维词向量表示\n",
    "    input = t.arange(0, 6).view(3, 2).long()  # 3个句子，每句子有2个词\n",
    "    input = t.autograd.Variable(input)\n",
    "    output = embedding(input)\n",
    "    print(output.size())\n",
    "    print(embedding.weight.size())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
