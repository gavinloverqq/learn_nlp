{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者：孙文奇\n",
    "链接：https://zhuanlan.zhihu.com/p/361431647\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "在PyTorch中，针对词向量有一个专门的层nn.Embedding，用来实现词与词向量的映射。nn.Embedding具有一个权重（.weight），形状是(num_words, embedding_dim)。例如一共有10个词，每个词用2维向量表征，对应的权重就是一个10×2的矩阵。Embedding的输入形状N×W，N是batch size，W是序列的长度，输出的形状是N×W×embedding_dim。输入必须是LongTensor，FloatTensor需通过tensor.long()方法转成LongTensor。Embedding的权重是可以训练的，既可以采用随机初始化，也可以采用预训练好的词向量初始化。# coding:utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embedding = nn.Embedding(10, 2)  # 10个词，每个词用2维词向量表示\n",
    "    input = t.arange(0, 6).view(3, 2).long()  # 3个句子，每句子有2个词\n",
    "    input = t.autograd.Variable(input)\n",
    "    output = embedding(input)\n",
    "    print(output.size())\n",
    "    print(embedding.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch中的transpose和view的区别\n",
    "\n",
    "作者：随疯\n",
    "链接：https://zhuanlan.zhihu.com/p/578351784\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "先上结论：transpose方法和view方法组成的同size的矩阵值并不相同！在实际应用中，经常会遇到需要对张量维度进行变换的情况，经常用到的就是transpose方法和view方法。但是这两种方法有着本质的区别，使用错误会导致难以察觉的错误。transpose方法transpose方法可以理解为维度交换，也就是转置。它有两个参数dim1和dim2，意义很简单，即交换dim1和dim2两个维度。view方法view方法则可以理解成对张量重新进行维度定义，它只有一个参数size，但这个size可以是元组或者列表，表示重新进行定义的维度。两者的区别具体看一个例子。import torch\n",
    "\n",
    "a = torch.tensor([[[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9],\n",
    "                   [10, 11, 12]]])\n",
    "print(a.shape)这里我们定义了一个张量a，它的维度是（1，4，3），然后对比一下两个方法的区别：b = a.transpose(1, 2)\n",
    "c = a.view((1, 3, -1))\n",
    "print(\"Here is Tensor b:\\n\")\n",
    "print(b)\n",
    "print(b.shape)\n",
    "print(\"Here is Tensor c:\\n\")\n",
    "print(c)\n",
    "print(c.shape)输出结果如下图很明显，transpose方法对a的后两维进行了转置交换，而view方法则是以行序对所有元素重新设定维度。因此在实际应用时一定要注意自己需要实现怎样的维度变换，是交换两个维度还是重新定义维度，一旦误用很难发现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3])\n",
      "Here is Tensor b:\n",
      "\n",
      "tensor([[[ 1,  4,  7, 10],\n",
      "         [ 2,  5,  8, 11],\n",
      "         [ 3,  6,  9, 12]]])\n",
      "torch.Size([1, 3, 4])\n",
      "Here is Tensor c:\n",
      "\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]]])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9],\n",
    "                   [10, 11, 12]]])\n",
    "print(a.shape)\n",
    "\n",
    "b = a.transpose(1, 2)\n",
    "c = a.view((1, 3, -1))\n",
    "print(\"Here is Tensor b:\\n\")\n",
    "print(b)\n",
    "print(b.shape)\n",
    "print(\"Here is Tensor c:\\n\")\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
